{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680fd3f0-ea3f-48de-b072-4c4b3c299f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pc/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pc/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9d58e9c3-2906-4e1a-8bb3-3c5dfabd9d98;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 125ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9d58e9c3-2906-4e1a-8bb3-3c5dfabd9d98\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 05:20:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3\")\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/home/pc/TestJupyter/opt/spark-3.3.0/venv-spark/bin/python39\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.3.0  pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.driver.memory\", \"70g\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://10.123.51.78:8000/user/hive/warehouse\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://g2.bigtop.it:9083\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def read_avro(path):\n",
    "    return spark.read.format(\"avro\").load(path)\n",
    "\n",
    "def write_avro(df, path):\n",
    "    return df.write.format(\"avro\").save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af05b274-ba40-4367-a8a8-c2685be81c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(query):\n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8af5ca2-4172-4052-bc7b-f15050bd3382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+-----------+\n",
      "|namespace|tableName                  |isTemporary|\n",
      "+---------+---------------------------+-----------+\n",
      "|default  |books_ext                  |false      |\n",
      "|default  |soc_lang_gram_frequency    |false      |\n",
      "|default  |soc_tag_gram_frequency     |false      |\n",
      "|default  |soc_token_gram_frequency   |false      |\n",
      "|default  |socialmediangrams          |false      |\n",
      "|default  |transaction_detail_hive_tbl|false      |\n",
      "|default  |wiki                       |false      |\n",
      "|default  |wiki_tag_gram_frequency    |false      |\n",
      "|default  |wiki_token_gram_frequency  |false      |\n",
      "+---------+---------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q(\"show tables;\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e6dcaa-2e69-418c-9640-ab20891beac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+---------------------------------+----------+------------+----------+---------+-----------+----------+--------+\n",
      "| sentence_id|pos|                       token_gram|tok_gram_f|    tag_gram|tag_gram_f|gram_type|containsKey|middle_key|language|\n",
      "+------------+---+---------------------------------+----------+------------+----------+---------+-----------+----------+--------+\n",
      "|146028889542|  3|             香港 的 一个 公共 屋|        37| ns uj m b n|        87|        5|       true|     false| chinese|\n",
      "|        4002|  2|                        台湾 棒球|       230|        ns n|    170697|        2|       true|     false| chinese|\n",
      "|403726929223|  4|                      中国 经济网|       325|        ns n|    170697|        2|       true|     false| chinese|\n",
      "|429496729693| 56|                        中国 一同|         2|        ns d|      8698|        2|       true|     false| chinese|\n",
      "|309237646939| 20|                   香港 音乐 乐坛|         1|      ns n n|     61819|        3|       true|     false| chinese|\n",
      "|455266537091| 21|           中国 好 歌曲 比赛 盲选|         1| ns a n vn v|         7|        5|       true|     false| chinese|\n",
      "|137438959848|  2|                   台湾 的 漫画家|        29|     ns uj n|      9194|        3|       true|     false| chinese|\n",
      "| 42949673295| 14|         中国 全国 家 电视台 播出|         1|  ns n q n v|        17|        5|       true|     false| chinese|\n",
      "| 51539608154| 12|                   中国 首部 神话|         1|      ns m n|      3916|        3|       true|     false| chinese|\n",
      "|231928237946|  9|             台湾 渔船 曾 在 关岛|         1| ns n d p ns|       104|        5|       true|     false| chinese|\n",
      "|120259087811|  3|              香港 国际 影视 展部|        21|    ns n n n|     19649|        4|       true|     false| chinese|\n",
      "|257698043127|  9|                        台湾 大戟|         1|        ns n|    170697|        2|       true|     false| chinese|\n",
      "|472446402650|  5|   中国 天津市 和平区 劝业场 街道|         1|ns ns ns n n|       632|        5|       true|     false| chinese|\n",
      "|266287975132|  0|                   香港 复印 授权|         2|      ns v v|      3575|        3|       true|     false| chinese|\n",
      "|403726931470|  6|中国 伊斯兰教 学者 现任 中国伊...|         2|ns nz n n nt|        26|        5|       true|     false| chinese|\n",
      "|274877907322|  6|               中国 龙艺 之 乡 的|         1|ns nr u n uj|         5|        5|       true|     false| chinese|\n",
      "| 68719477564| 14|                香港 的 立场 彭博|         1|  ns uj n nr|       171|        4|       true|     false| chinese|\n",
      "|249108105573|  2|                   台湾 音乐 中心|         2|      ns n n|     61819|        3|       true|     false| chinese|\n",
      "|369367188007| 14|              台湾 中部 地区 重要|         4|    ns f n a|        45|        4|       true|     false| chinese|\n",
      "|163208758214| 19|           香港 四家 拥有 频谱 的|         1| ns m v n uj|        50|        5|       true|     false| chinese|\n",
      "+------------+---+---------------------------------+----------+------------+----------+---------+-----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"avro\").load(\"output/28_TagGramF\")\n",
    "\n",
    "\n",
    "gram_table = df.select(\"sentence_id\", \"pos\", \"token_gram\", \"tok_gram_f\", \"tag_gram\", \"tag_gram_f\", \"gram_type\", \"containsKey\", \"middle_key\", \"language\")\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree(\"30_GRAM_TABLE\")\n",
    "\n",
    "write_avro(gram_table, \"30_GRAM_TABLE\")\n",
    "gram_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900f3301-0c3c-455c-adc4-83405a3bbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"avro\").load(\"output/28_TagGramF\")\n",
    "df = df.repartition(800)\n",
    "originalSentence  = df.dropDuplicates((['sentence_id'])).select(\"sentence_id\", \"original\", \"tag\", \"language\", \"token\", \"normal_tag\")\n",
    "originalSentence = originalSentence.repartition(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7469b7d1-01be-4ea6-ac05-6bcdbd35a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(\"output/29_ORIGINAL_DATA\")\n",
    "except:\n",
    "    pass\n",
    "write_avro(originalSentence, \"output/29_ORIGINAL_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f702fda0-761e-46db-a1c1-3865962bb7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------+--------------------+--------+----------------------------+--------------------+\n",
      "| sentence_id|                             original|                 tag|language|                       token|          normal_tag|\n",
      "+------------+-------------------------------------+--------------------+--------+----------------------------+--------------------+\n",
      "|128849027708|                 the administratio...|[DET, NOUN, ADP, ...| english|        [the, administrat...|                null|\n",
      "|481036343692|                 west african affa...|[ADV, ADJ, NOUN, ...| english|        [west, african, a...|                null|\n",
      "|128849030108|                 fisher went on to...|[ADV, VERB, ADP, ...| english|        [fisher, went, on...|                null|\n",
      "|438086674396|                 in the school of ...|[ADP, DET, NOUN, ...| english|        [in, the, school,...|                null|\n",
      "|438086676944|                 memikirkan betapa...|[VERB, ADV, ADJ, ...|   malay|        [memikirkan, beta...|                null|\n",
      "|360777265964|                 maraimalainagar i...|[NOUN, AUX, DET, ...|   malay|        [maraimalainagar,...|                null|\n",
      "|317827591708|                 dalam tahun perta...|[ADP, NOUN, NUM, ...|   malay|        [dalam, tahun, pe...|                null|\n",
      "|386547066483|                 são paulo still l...|[NOUN, NOUN, ADV,...| english|        [são, paulo, stil...|                null|\n",
      "| 77309411678|       早年 由于日本女子组合 morni...|[t, c, ns, n, v, ...| chinese|[早年, 由于, 日本, 女子, ...|[t, CONJ, PROPN, ...|\n",
      "| 60129552533|                 john f kennedy me...|[PROPN, ADP, PROP...| english|        [john, f, kennedy...|                null|\n",
      "|317827582527| 月日 中国首都北京天安门广场发生自...|[m, m, ns, d, ns,...| chinese|[月, 日, 中国, 首都, 北京...|[m, m, PROPN, ADV...|\n",
      "|335007461197|                 pommiers rhône ia...|[NOUN, PROPN, AUX...|   malay|        [pommiers, rhone,...|                null|\n",
      "|335007455451|   光绪二十一年 年 中 日甲午战争所...|[n, m, m, f, m, n...| chinese|[光绪, 二十一年, 年, 中, ...|[NOUN, m, m, f, m...|\n",
      "|446676611215|                 ia juga merupakan...|[PRON, ADV, VERB,...|   malay|        [ia, juga, merupa...|                null|\n",
      "|240518169857|  年 经民国实业部举荐 兼任中国酒精...|[m, n, n, n, v, v...| chinese|[年, 经, 民国, 实业部, 举...|[m, NOUN, NOUN, N...|\n",
      "|360777265092|                 majhakot merupaka...|[NOUN, VERB, DET,...|   malay|        [majhakot, merupa...|                null|\n",
      "|180388638317|                 ialah komun di ja...|[AUX, NOUN, ADP, ...|   malay|        [ialah, komun, di...|                null|\n",
      "|403726931762|朝庸溪为一位于台湾东部之县市管河川...|[ns, p, m, v, ns,...| chinese|[朝庸溪, 为, 一, 位于, 台...|[PROPN, ADP, m, V...|\n",
      "|360777263792|                 chapel the univer...|[VERB, DET, NOUN,...| english|        [chapel, the, uni...|                null|\n",
      "|274877911916| 商汤集团于月日宣布 全球发售及上市...|[n, n, p, m, m, v...| chinese| [商汤, 集团, 于, 月, 日,...|[NOUN, NOUN, ADP,...|\n",
      "+------------+-------------------------------------+--------------------+--------+----------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_avro(\"output/29_ORIGINAL_DATA\").show()\n",
    "\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b089a-473f-44d3-b683-c213920466bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a26763-3210-40a6-846f-842b7f1125e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: java.io.IOException Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length; Host Details : local host is: \"g2.bigtop.it/10.123.51.78\"; destination host is: \"g2.bigtop.it\":8000; )",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreate database wikipedia;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: java.io.IOException Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length; Host Details : local host is: \"g2.bigtop.it/10.123.51.78\"; destination host is: \"g2.bigtop.it\":8000; )"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create database wikipedia;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f03ad84-8f27-41ab-b1a5-e7c0718652a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+-----------+\n",
      "|namespace|tableName                  |isTemporary|\n",
      "+---------+---------------------------+-----------+\n",
      "|default  |books_ext                  |false      |\n",
      "|default  |soc_lang_gram_frequency    |false      |\n",
      "|default  |soc_tag_gram_frequency     |false      |\n",
      "|default  |soc_token_gram_frequency   |false      |\n",
      "|default  |socialmediangrams          |false      |\n",
      "|default  |source_data                |false      |\n",
      "|default  |transaction_detail_hive_tbl|false      |\n",
      "|default  |wiki                       |false      |\n",
      "|default  |wiki_gram                  |false      |\n",
      "|default  |wiki_tag_gram_frequency    |false      |\n",
      "|default  |wiki_token_gram_frequency  |false      |\n",
      "+---------+---------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables;\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc9c03c-7988-4eb9-9b89-de06be82e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table source_data;\")\n",
    "\n",
    "read_avro(\"output/29_ORIGINAL_DATA\").write\\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"source_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2c9c3c-5cca-454c-af57-0c0d98011491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table wiki_gram;\")\n",
    "\n",
    "\n",
    "read_avro(\"30_GRAM_TABLE\").write\\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"wiki_gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c434660-8801-4f91-a23b-1e02a135a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(\"use namespace default;\")\n",
    "q(\"drop table soc_lang_gram_frequency;\")\n",
    "q(\"drop table soc_tag_gram_frequency;\")\n",
    "q(\"drop table soc_token_gram_frequency;\")\n",
    "q(\"drop table socialmediangrams;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79be15cd-cbd3-4212-a51d-77da8b32c9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|           books_ext|      false|\n",
      "|  default|         source_data|      false|\n",
      "|  default|transaction_detai...|      false|\n",
      "|  default|           wiki_gram|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q(\"show tables;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54b341-8e07-46bd-8d85-b15c494468d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-spark",
   "language": "python",
   "name": "venv-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
