{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c32340a-b7bc-45fb-9f27-f4a36d7bc9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pc/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pc/.ivy2/jars\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-35563898-7829-4674-8739-e5988dc4d9ae;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.3.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 124ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.spark#spark-avro_2.12;3.3.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-35563898-7829-4674-8739-e5988dc4d9ae\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 19:08:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/08 19:08:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/08 19:08:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/08 19:08:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3\")\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/home/pc/TestJupyter/opt/spark-3.3.0/venv-spark/bin/python39\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.3.0  pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.driver.memory\", \"60g\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://10.123.51.78:8000/user/hive/warehouse\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://g2.bigtop.it:9083\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c1cd0c-edc0-4dbf-bb5b-e860677306bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(query):\n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bc773b-0ab2-4256-9ed6-4aea500bc260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+-----------+\n",
      "|namespace|tableName                  |isTemporary|\n",
      "+---------+---------------------------+-----------+\n",
      "|default  |books_ext                  |false      |\n",
      "|default  |source_data                |false      |\n",
      "|default  |tag_gram_ranking           |false      |\n",
      "|default  |tag_gram_ranking_small     |false      |\n",
      "|default  |transaction_detail_hive_tbl|false      |\n",
      "|default  |wiki_gram                  |false      |\n",
      "+---------+---------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q(\"show tables;\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e776aaa8-e56f-467e-858e-26a023a7bc08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 14:31:42 WARN DropTableCommand: org.apache.spark.sql.AnalysisException: Table or view not found: top100_3_gram; line 1 pos 14\n",
      "org.apache.spark.sql.AnalysisException: Table or view not found: top100_3_gram; line 1 pos 14\n",
      "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:101)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1017)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1020)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.$anonfun$applyOrElse$47(Analyzer.scala:1068)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1068)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n",
      "\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:604)\n",
      "\tat org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:238)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "14:31:42.648 [Thread-5] ERROR hive.log - error in initSerDe: java.lang.ClassNotFoundException Class org.apache.hadoop.hive.hbase.HBaseSerDe not found\n",
      "java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.hbase.HBaseSerDe not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592) ~[hadoop-client-api-3.3.2.jar:?]\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:447) ~[hive-metastore-2.3.9.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:440) ~[hive-metastore-2.3.9.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree2$1(HiveClientImpl.scala:437) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:436) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:430) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:430) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:428) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:90) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:89) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:92) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:122) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:729) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:729) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:515) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:66) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:311) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.Option.orElse(Option.scala:447) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1136) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1094) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.DropTable.mapChildren(v2Commands.scala:661) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]\n",
      "14:31:42.661 [Thread-5] ERROR hive.ql.metadata.Table - Unable to get field from serde: org.apache.hadoop.hive.hbase.HBaseSerDe\n",
      "java.lang.RuntimeException: MetaException(message:java.lang.ClassNotFoundException Class org.apache.hadoop.hive.hbase.HBaseSerDe not found)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:283) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree2$1(HiveClientImpl.scala:437) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:436) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:430) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:430) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:428) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:90) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:89) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:92) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:122) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:729) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:729) ~[spark-hive_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:515) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:66) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:311) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.Option.orElse(Option.scala:447) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1136) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1094) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.DropTable.mapChildren(v2Commands.scala:661) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617) ~[spark-sql_2.12-3.3.0.jar:3.3.0]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_342]\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_342]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_342]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_342]\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_342]\n",
      "Caused by: org.apache.hadoop.hive.metastore.api.MetaException: java.lang.ClassNotFoundException Class org.apache.hadoop.hive.hbase.HBaseSerDe not found\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:461) ~[hive-metastore-2.3.9.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:440) ~[hive-metastore-2.3.9.jar:2.3.9]\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) ~[hive-exec-2.3.9-core.jar:2.3.9]\n",
      "\t... 95 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.sql.\n: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.hbase.HBaseStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:297)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:471)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:430)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:430)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:428)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:90)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:89)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:92)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:122)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:729)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:729)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:515)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:66)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1138)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1136)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1094)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)\n\tat org.apache.spark.sql.catalyst.plans.logical.DropTable.mapChildren(v2Commands.scala:661)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.hbase.HBaseStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:310)\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:292)\n\t... 91 more\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.hbase.HBaseStorageHandler\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:305)\n\t... 92 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m q(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop view top100_5_gram;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m q(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop view top100_overall_gram;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop table transaction_detail_hive_tbl;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m q(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop table transaction_detail_hive_tbl;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mq\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mq\u001b[39m(query):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35.sql.\n: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.hbase.HBaseStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:297)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:471)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:430)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:430)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:428)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:90)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:89)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:92)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:122)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:729)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:729)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:515)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:66)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupTableOrView$2(Analyzer.scala:1138)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableOrView(Analyzer.scala:1136)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1094)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)\n\tat org.apache.spark.sql.catalyst.plans.logical.DropTable.mapChildren(v2Commands.scala:661)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.hbase.HBaseStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:310)\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:292)\n\t... 91 more\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.hbase.HBaseStorageHandler\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:305)\n\t... 92 more\n"
     ]
    }
   ],
   "source": [
    "q(\"drop view top100_2_gram;\")\n",
    "q(\"drop view top100_4_gram;\")\n",
    "q(\"drop view top100_5_gram;\")\n",
    "q(\"drop view top100_overall_gram;\")\n",
    "q(\"drop table transaction_detail_hive_tbl;\")\n",
    "q(\"drop table transaction_detail_hive_tbl;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe3a16a-ff06-4833-98e5-42db0dba2fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|social_media|\n",
      "|         xyz|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "138e5758-85ba-4a3c-87a4-336c268a5c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(\"use social_media;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3308c4-0f06-4805-8aeb-3f3b958c7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e9310d-5ea3-4faf-99ba-edb611116f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = spark.read.format(\"avro\").load(\"output/29_ORIGINAL_DATA\")\n",
    "gram_table = spark.read.format(\"avro\").load(\"output/28_ALL_FREQ_INCLUDED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc46e39f-bf1c-4ca9-866d-d79e9c097f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|social_media|\n",
      "|         xyz|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b58a80-9db7-475c-9c3f-f56cdea607a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = original.withColumn(\"unique_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb0087b-7499-4503-986a-9cadb352324e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------+--------------------+--------+----------------------------+--------------------+--------------------+---------+\n",
      "| sentence_id|                           original|                 tag|language|                       token|         chinese_tag|    language_pattern|unique_id|\n",
      "+------------+-----------------------------------+--------------------+--------+----------------------------+--------------------+--------------------+---------+\n",
      "| 42949733892|               its cheap and not...|[PROPN, ADJ, CCON...| english|        [its, cheap, and,...|                null|[en, en, en, en, ...|        0|\n",
      "| 34359770525|               barang dah selama...|[NOUN, ADV, ADJ, ...|   malay|        [barang, dah, sel...|                null|[ms, ms, ms, ms, ...|        1|\n",
      "|  8589985135|               mantap terbaik pe...|[NOUN, ADJ, NOUN,...|   malay|        [mantap, terbaik,...|                null|[ms, ms, ms, ms, ...|        2|\n",
      "|       42514|               nice with colour ...|[ADJ, ADP, NOUN, ...| english|        [nice, with, colo...|                null|[en, en, en, en, ...|        3|\n",
      "|128849025325|               item received in ...|[ADJ, VERB, ADP, ...| english|        [item, received, ...|                null|[ms, en, en, en, ...|        4|\n",
      "|  8589947001|               dapat barang dala...|[VERB, NOUN, ADP,...|   malay|        [dapat, barang, d...|                null|[ms, ms, ms, ms, ...|        5|\n",
      "|  8589937511|               terima kasih sall...|[NOUN, NOUN, NOUN...|   malay|        [terima, kasih, s...|                null|[ms, ms, en, ms, ...|        6|\n",
      "| 42949717559|               baguih la pampers...|[VERB, DET, NOUN,...|   malay|        [baguih, la, pamp...|                null|[ms, ms, en, ms, ...|        7|\n",
      "|120259111446|               my rd purchase al...|[NOUN, NOUN, NOUN...| english|        [my, rd, purchase...|                null|[en, en, en, en, ...|        8|\n",
      "| 68719479199|               alhamdulillah so ...|[ADV, ADV, NOUN, ...|   malay|        [alhamdulillah, s...|                null|[ms, en, en, ms, ...|        9|\n",
      "|120259125051|               comfortable and n...|[ADJ, CCONJ, ADJ,...| english|        [comfortable, and...|                null|[en, en, en, en, ...|       10|\n",
      "|137438989437|               otherwise very sa...|[ADV, VERB, PROPN...| english|        [otherwise, very,...|                null|[en, en, en, en, ...|       11|\n",
      "|111669193195|               thank you seller ...|[VERB, PRON, PROP...| english|        [thank, you, sell...|                null|[en, en, en, en, ...|       12|\n",
      "|103079262161|               actually order he...|[ADV, NOUN, NOUN,...| english|        [actually, order,...|                null|[en, en, ms, ms, ...|       13|\n",
      "|137439012173|               look nice still a...|[VERB, ADJ, ADV, ...| english|        [look, nice, stil...|                null|[en, en, en, en, ...|       14|\n",
      "|120259121864|               cute the seller e...|[ADJ, DET, NOUN, ...| english|        [cute, the, selle...|                null|[en, en, en, en, ...|       15|\n",
      "|154618834071|发货速度很快 包装也很好 期待下次...|[NOUN, NOUN, ADV,...| chinese|[发货, 速度, 很快, 包装, ...|[n, n, d, v, d, d...|[zh, zh, zh, zh, ...|       16|\n",
      "| 51539628146|               nice material bea...|[ADJ, NOUN, PROPN...| english|        [nice, material, ...|                null|[en, en, en, en, ...|       17|\n",
      "| 60129566229|               ok barang da samp...|[ADJ, NOUN, ADV, ...|   malay|        [ok, barang, da, ...|                null|[ms, ms, ms, ms, ...|       18|\n",
      "|  8590001473|               oklah hitam pun c...|[NOUN, ADJ, PART,...|   malay|        [oklah, hitam, pu...|                null|[ms, ms, ms, ms, ...|       19|\n",
      "+------------+-----------------------------------+--------------------+--------+----------------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353bba09-eded-4396-9a7d-19262d0dc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_table = gram_table.withColumn(\"unique_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685cf94e-f519-406e-840b-0b3bedacd39b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------+--------------------+-----------+--------------------+---+------------------+-----------------+---------+---------+-----------+----------+-----------+------------+----------+---------+\n",
      "| sentence_id|            original|                 tag|language|               token|chinese_tag|    language_pattern|pos|        token_gram|         tag_gram|lang_gram|gram_type|containsKey|middle_key|lang_gram_f|token_gram_f|tag_gram_f|unique_id|\n",
      "+------------+--------------------+--------------------+--------+--------------------+-----------+--------------------+---+------------------+-----------------+---------+---------+-----------+----------+-----------+------------+----------+---------+\n",
      "| 42949713335|barang sampai dal...|[NOUN, VERB, ADP,...|   malay|[barang, sampai, ...|       null|[ms, ms, ms, ms, ...|  0|     barang sampai|        NOUN VERB|    ms ms|        2|       true|     false|     486008|       61098|    118346|        0|\n",
      "|        7613|thankyouuuuu bara...|[NOUN, NOUN, ADV,...|   malay|[thankyouuuuu, ba...|       null|[en, ms, ms, ms, ...|  1|        barang dah|         NOUN ADV|    ms ms|        2|       true|     false|     486008|       41968|    149859|        1|\n",
      "| 85899400519|maaf gambar tiada...|[NOUN, NOUN, PART...|   malay|[maaf, gambar, ti...|       null|[ms, ms, ms, ms, ...|  4|     barang sampai|         NOUN ADP|    ms ms|        2|       true|     false|     486008|       61098|     94685|        2|\n",
      "| 25769847775|alhamdulilah bara...|[NOUN, NOUN, ADP,...|   malay|[alhamdulilah, ba...|       null|[ms, ms, ms, ms, ...|  1|     barang sampai|         NOUN ADP|    ms ms|        2|       true|     false|     486008|       61098|     94685|        3|\n",
      "| 42949731929|thank seller bara...|[NOUN, NOUN, NOUN...|   malay|[thank, seller, b...|       null|[en, en, ms, ms, ...|  2| barang dah sampai|    NOUN ADV VERB| ms ms ms|        3|       true|     false|     394027|       29908|    179518|        4|\n",
      "| 34359746622|barang sampai den...|[NOUN, VERB, ADP,...|   malay|[barang, sampai, ...|       null|[ms, ms, ms, ms, ...|  0|     barang sampai|        NOUN VERB|    ms ms|        2|       true|     false|     486008|       61098|    118346|        5|\n",
      "| 34359786323|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0| barang dah sampai|    NOUN ADV VERB| ms ms ms|        3|       true|     false|     394027|       29908|    179518|        6|\n",
      "|137439019096|nice nice nice ni...|[PROPN, PROPN, PR...| english|[nice, nice, nice...|       null|[en, en, en, en, ...|  6|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|        7|\n",
      "| 42949675841|alhamdulillah dah...|[ADV, ADV, VERB, ...|   malay|[alhamdulillah, d...|       null|[ms, ms, ms, en, ...|  7|   seller siap dpt|    NOUN ADJ VERB| en ms en|        3|       true|     false|      80693|          87|     33380|        8|\n",
      "| 34359754309|mask dia bagus se...|[NOUN, PRON, ADJ,...|   malay|[mask, dia, bagus...|       null|[ms, ms, ms, ms, ...|  9|   murah pon murah|      ADJ ADV ADJ| ms en ms|        3|       true|     false|      60850|         594|     20568|        9|\n",
      "| 34359746281|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0| barang dah sampai|    NOUN ADV VERB| ms ms ms|        3|       true|     false|     394027|       29908|    179518|       10|\n",
      "| 60129563098|barang sampai dal...|[NOUN, VERB, ADP,...|   malay|[barang, sampai, ...|       null|[ms, ms, ms, ms, ...|  0|     barang sampai|        NOUN VERB|    ms ms|        2|       true|     false|     486008|       61098|    118346|       11|\n",
      "| 34359740086|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0|        barang dah|         NOUN ADV|    ms ms|        2|       true|     false|     486008|       41968|    149859|       12|\n",
      "| 25769835845|penghantaran laju...|[NOUN, NOUN, NOUN...|   malay|[penghantaran, la...|       null|[ms, ms, ms, ms, ...| 20|  seller tq seller|  NOUN PROPN NOUN| en en en|        3|       true|     false|     405635|        6567|     31644|       13|\n",
      "| 51539673266|nice nice nice ni...|[PROPN, PROPN, PR...| english|[nice, nice, nice...|       null|[en, en, en, en, ...| 14|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|       14|\n",
      "|       81870|nice nice nice ni...|[PROPN, PROPN, PR...| english|[nice, nice, nice...|       null|[en, en, en, en, ...|  0|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|       15|\n",
      "|       76849|fast delivery qua...|[PROPN, NOUN, PRO...| english|[fast, delivery, ...|       null|[en, en, en, ms, ...|  8|seller recommended|      PROPN PROPN|    en en|        2|       true|     false|     503440|        3127|    105101|       16|\n",
      "|  8589941162|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0|        barang dah|         NOUN ADV|    ms ms|        2|       true|     false|     486008|       41968|    149859|       17|\n",
      "|  8589994790|nice nice nice go...|[PROPN, PROPN, PR...|   malay|[nice, nice, nice...|       null|[en, en, en, en, ...|  8|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|       18|\n",
      "|111669195748|semua barang samp...|[DET, NOUN, ADP, ...|   malay|[semua, barang, s...|       null|[ms, ms, ms, ms, ...|  1|     barang sampai|         NOUN ADP|    ms ms|        2|       true|     false|     486008|       61098|     94685|       19|\n",
      "+------------+--------------------+--------------------+--------+--------------------+-----------+--------------------+---+------------------+-----------------+---------+---------+-----------+----------+-----------+------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gram_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d27c5-9826-4865-9c5d-955836c9aa74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001364b8-4272-4d4c-9412-0bd3c377e61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(\"drop table source_data;\")\n",
    "q(\"drop table gram_table;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa7743bb-04ef-4754-9b3d-fbc828c552e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: gram_table; line 1 pos 11;\n'DropTable false, false\n+- 'UnresolvedTableOrView [gram_table], DROP TABLE, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop table gram_table;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mq\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mq\u001b[39m(query):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: gram_table; line 1 pos 11;\n'DropTable false, false\n+- 'UnresolvedTableOrView [gram_table], DROP TABLE, true\n"
     ]
    }
   ],
   "source": [
    "q(\"drop table gram_table;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2fd6f1-487d-4285-b08b-f3bb6e99051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "original.write\\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"social_media.source_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43977cca-8624-4286-b98b-5960fc8418e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|   namespace|           tableName|isTemporary|\n",
      "+------------+--------------------+-----------+\n",
      "|social_media|   lang_gram_ranking|      false|\n",
      "|social_media|lang_gram_ranking...|      false|\n",
      "|social_media|         source_data|      false|\n",
      "|social_media|    tag_gram_ranking|      false|\n",
      "|social_media|tag_gram_ranking_...|      false|\n",
      "+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q(\"use social_media;\")\n",
    "q(\"show tables;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f0d0e8-2981-472e-a97e-15a9beaca67b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: gram_table; line 1 pos 11;\n'DropTable false, false\n+- 'UnresolvedTableOrView [gram_table], DROP TABLE, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop table gram_table;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mq\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mq\u001b[39m(query):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: gram_table; line 1 pos 11;\n'DropTable false, false\n+- 'UnresolvedTableOrView [gram_table], DROP TABLE, true\n"
     ]
    }
   ],
   "source": [
    "q(\"drop table gram_table;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cec2b0-fa1d-40b4-88ec-27084a8fc860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gram_table.write\\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"gram_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c67aba-95aa-45e9-8cc8-3ca3798fa904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================>                                   (10 + 17) / 27]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------+--------------------+-----------+--------------------+---+------------------+-----------------+---------+---------+-----------+----------+-----------+------------+----------+---------+\n",
      "| sentence_id|            original|                 tag|language|               token|chinese_tag|    language_pattern|pos|        token_gram|         tag_gram|lang_gram|gram_type|containsKey|middle_key|lang_gram_f|token_gram_f|tag_gram_f|unique_id|\n",
      "+------------+--------------------+--------------------+--------+--------------------+-----------+--------------------+---+------------------+-----------------+---------+---------+-----------+----------+-----------+------------+----------+---------+\n",
      "| 42949713335|barang sampai dal...|[NOUN, VERB, ADP,...|   malay|[barang, sampai, ...|       null|[ms, ms, ms, ms, ...|  0|     barang sampai|        NOUN VERB|    ms ms|        2|       true|     false|     486008|       61098|    118346|        0|\n",
      "|        7613|thankyouuuuu bara...|[NOUN, NOUN, ADV,...|   malay|[thankyouuuuu, ba...|       null|[en, ms, ms, ms, ...|  1|        barang dah|         NOUN ADV|    ms ms|        2|       true|     false|     486008|       41968|    149859|        1|\n",
      "| 85899400519|maaf gambar tiada...|[NOUN, NOUN, PART...|   malay|[maaf, gambar, ti...|       null|[ms, ms, ms, ms, ...|  4|     barang sampai|         NOUN ADP|    ms ms|        2|       true|     false|     486008|       61098|     94685|        2|\n",
      "| 25769847775|alhamdulilah bara...|[NOUN, NOUN, ADP,...|   malay|[alhamdulilah, ba...|       null|[ms, ms, ms, ms, ...|  1|     barang sampai|         NOUN ADP|    ms ms|        2|       true|     false|     486008|       61098|     94685|        3|\n",
      "| 42949731929|thank seller bara...|[NOUN, NOUN, NOUN...|   malay|[thank, seller, b...|       null|[en, en, ms, ms, ...|  2| barang dah sampai|    NOUN ADV VERB| ms ms ms|        3|       true|     false|     394027|       29908|    179518|        4|\n",
      "| 34359746622|barang sampai den...|[NOUN, VERB, ADP,...|   malay|[barang, sampai, ...|       null|[ms, ms, ms, ms, ...|  0|     barang sampai|        NOUN VERB|    ms ms|        2|       true|     false|     486008|       61098|    118346|        5|\n",
      "| 34359786323|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0| barang dah sampai|    NOUN ADV VERB| ms ms ms|        3|       true|     false|     394027|       29908|    179518|        6|\n",
      "|137439019096|nice nice nice ni...|[PROPN, PROPN, PR...| english|[nice, nice, nice...|       null|[en, en, en, en, ...|  6|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|        7|\n",
      "| 42949675841|alhamdulillah dah...|[ADV, ADV, VERB, ...|   malay|[alhamdulillah, d...|       null|[ms, ms, ms, en, ...|  7|   seller siap dpt|    NOUN ADJ VERB| en ms en|        3|       true|     false|      80693|          87|     33380|        8|\n",
      "| 34359754309|mask dia bagus se...|[NOUN, PRON, ADJ,...|   malay|[mask, dia, bagus...|       null|[ms, ms, ms, ms, ...|  9|   murah pon murah|      ADJ ADV ADJ| ms en ms|        3|       true|     false|      60850|         594|     20568|        9|\n",
      "| 34359746281|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0| barang dah sampai|    NOUN ADV VERB| ms ms ms|        3|       true|     false|     394027|       29908|    179518|       10|\n",
      "| 60129563098|barang sampai dal...|[NOUN, VERB, ADP,...|   malay|[barang, sampai, ...|       null|[ms, ms, ms, ms, ...|  0|     barang sampai|        NOUN VERB|    ms ms|        2|       true|     false|     486008|       61098|    118346|       11|\n",
      "| 34359740086|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0|        barang dah|         NOUN ADV|    ms ms|        2|       true|     false|     486008|       41968|    149859|       12|\n",
      "| 25769835845|penghantaran laju...|[NOUN, NOUN, NOUN...|   malay|[penghantaran, la...|       null|[ms, ms, ms, ms, ...| 20|  seller tq seller|  NOUN PROPN NOUN| en en en|        3|       true|     false|     405635|        6567|     31644|       13|\n",
      "| 51539673266|nice nice nice ni...|[PROPN, PROPN, PR...| english|[nice, nice, nice...|       null|[en, en, en, en, ...| 14|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|       14|\n",
      "|       81870|nice nice nice ni...|[PROPN, PROPN, PR...| english|[nice, nice, nice...|       null|[en, en, en, en, ...|  0|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|       15|\n",
      "|       76849|fast delivery qua...|[PROPN, NOUN, PRO...| english|[fast, delivery, ...|       null|[en, en, en, ms, ...|  8|seller recommended|      PROPN PROPN|    en en|        2|       true|     false|     503440|        3127|    105101|       16|\n",
      "|  8589941162|barang dah sampai...|[NOUN, ADV, VERB,...|   malay|[barang, dah, sam...|       null|[ms, ms, ms, ms, ...|  0|        barang dah|         NOUN ADV|    ms ms|        2|       true|     false|     486008|       41968|    149859|       17|\n",
      "|  8589994790|nice nice nice go...|[PROPN, PROPN, PR...|   malay|[nice, nice, nice...|       null|[en, en, en, en, ...|  8|    nice nice nice|PROPN PROPN PROPN| en en en|        3|       true|      true|     405635|       36717|    131744|       18|\n",
      "|111669195748|semua barang samp...|[DET, NOUN, ADP, ...|   malay|[semua, barang, s...|       null|[ms, ms, ms, ms, ...|  1|     barang sampai|         NOUN ADP|    ms ms|        2|       true|     false|     486008|       61098|     94685|       19|\n",
      "+------------+--------------------+--------------------+--------+--------------------+-----------+--------------------+---+------------------+-----------------+---------+---------+-----------+----------+-----------+------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q(\"select * from gram_table limit 100\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-spark",
   "language": "python",
   "name": "venv-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
