{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c074428b-d8e2-4986-98e7-b0ce34b37abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35386\n",
      "================ CONVERTING DATA TO AVRO FORMAT ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ SAVE THE PREPROCESSED DATA ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://10.123.51.78:8020/user/wikipedia/oriDataBM\n",
      "================ SAVE THE SENTENCE TOKENIZE DATA ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://10.123.51.78:8020/user/wikipedia/ppDataBM\n",
      "================ SAVE THE CLEANED SENTENCE DATA ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://10.123.51.78:8020/user/wikipedia/sent_tokenize_bm.avro\n",
      "================ SAVE THE FINAL PROCESS SENTENCE DATA ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://10.123.51.78:8020/user/wikipedia/p2_bm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.583 seconds.                                 (0 + 20) / 20]\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.583 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.586 seconds.\n",
      "Loading model cost 0.586 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.588 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.590 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.595 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.595 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.614 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.596 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.537 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.600 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.619 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.602 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.602 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.603 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.604 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.607 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.626 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.689 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/10 21:34:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/10 21:34:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/10 21:34:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+----------------+----------+----+\n",
      "|            word|word_count|  id|\n",
      "+----------------+----------+----+\n",
      "|        terletak|     10159|   1|\n",
      "|         kampung|      5445|   2|\n",
      "|          daerah|      5318|   3|\n",
      "|         sekolah|      4970|   4|\n",
      "|           komun|      4460|   5|\n",
      "|          bahasa|      3872|   6|\n",
      "|      kebangsaan|      3593|   7|\n",
      "|           filem|      3585|   8|\n",
      "|       digunakan|      2998|   9|\n",
      "|        perancis|      2839|  10|\n",
      "|     perbandaran|      2815|  11|\n",
      "|             dia|      2785|  12|\n",
      "|               é|      2768|  13|\n",
      "|            nama|      2718|  14|\n",
      "|           barat|      2350|  15|\n",
      "|        memiliki|      2280|  16|\n",
      "|           utara|      2239|  17|\n",
      "|            laut|      2080|  18|\n",
      "|        lapangan|      2079|  19|\n",
      "|               n|      2057|  20|\n",
      "|         tentera|      2054|  21|\n",
      "|            ahli|      2013|  22|\n",
      "|       kecamatan|      1968|  23|\n",
      "|            buah|      1961|  24|\n",
      "|          lelaki|      1937|  25|\n",
      "|          google|      1918|  26|\n",
      "|         seramai|      1901|  27|\n",
      "|            desa|      1850|  28|\n",
      "|           salah|      1806|  29|\n",
      "|           pulau|      1775|  30|\n",
      "|        dikenali|      1765|  31|\n",
      "|         pelajar|      1738|  32|\n",
      "|            lagu|      1735|  33|\n",
      "|        kemudian|      1671|  34|\n",
      "|         bancian|      1651|  35|\n",
      "|          melayu|      1643|  36|\n",
      "|            kota|      1597|  37|\n",
      "|          sungai|      1594|  38|\n",
      "|          jerman|      1559|  39|\n",
      "|            raja|      1482|  40|\n",
      "|          masjid|      1462|  41|\n",
      "|           jenis|      1448|  42|\n",
      "|          poskod|      1435|  43|\n",
      "|          carian|      1391|  44|\n",
      "|               á|      1338|  45|\n",
      "|             new|      1317|  46|\n",
      "|            bola|      1308|  47|\n",
      "|          perang|      1305|  48|\n",
      "|              de|      1304|  49|\n",
      "|          pemain|      1286|  50|\n",
      "|       kabupaten|      1284|  51|\n",
      "|           rasmi|      1271|  52|\n",
      "|            iran|      1270|  53|\n",
      "|           album|      1257|  54|\n",
      "|        sepanyol|      1249|  55|\n",
      "|               p|      1208|  56|\n",
      "|           namun|      1190|  57|\n",
      "|      universiti|      1182|  58|\n",
      "|               c|      1182|  59|\n",
      "|         januari|      1149|  60|\n",
      "|         setelah|      1134|  61|\n",
      "|            batu|      1123|  62|\n",
      "|            mula|      1110|  63|\n",
      "|           musim|      1105|  64|\n",
      "|         pelakon|      1098|  65|\n",
      "|           sepak|      1093|  66|\n",
      "|           kapal|      1067|  67|\n",
      "|          pernah|      1064|  68|\n",
      "|        provinsi|      1061|  69|\n",
      "|        inggeris|      1058|  70|\n",
      "|           kedah|      1046|  71|\n",
      "|        terkenal|      1038|  72|\n",
      "|            guru|      1023|  73|\n",
      "|          bentuk|      1017|  74|\n",
      "|          census|      1005|  75|\n",
      "|              tv|      1001|  76|\n",
      "|        biasanya|       991|  77|\n",
      "|         sekitar|       987|  78|\n",
      "|           turki|       986|  79|\n",
      "|          famili|       980|  80|\n",
      "|        november|       977|  81|\n",
      "|       perempuan|       973|  82|\n",
      "|           agama|       970|  83|\n",
      "|            buku|       970|  84|\n",
      "|            diri|       970|  85|\n",
      "|               b|       962|  86|\n",
      "|             ibu|       962|  87|\n",
      "|         sejarah|       959|  88|\n",
      "|         senarai|       958|  89|\n",
      "|           suatu|       951|  90|\n",
      "|            siri|       942|  91|\n",
      "|        menengah|       939|  92|\n",
      "|           biasa|       932|  93|\n",
      "|      azerbaijan|       925|  94|\n",
      "|        republik|       923|  95|\n",
      "|         oktober|       918|  96|\n",
      "|           mukim|       917|  97|\n",
      "|           laman|       916|  98|\n",
      "|      menjadikan|       912|  99|\n",
      "|           muzik|       911| 100|\n",
      "|        anugerah|       902| 101|\n",
      "|            data|       897| 102|\n",
      "|            lama|       893| 103|\n",
      "|     pentadbiran|       892| 104|\n",
      "|            asal|       892| 105|\n",
      "|           korea|       891| 106|\n",
      "|           meter|       890| 107|\n",
      "|      terengganu|       878| 108|\n",
      "|           lahir|       870| 109|\n",
      "|           bukit|       867| 110|\n",
      "|           zaman|       863| 111|\n",
      "|       wikipedia|       862| 112|\n",
      "|               e|       855| 113|\n",
      "|         terdiri|       852| 114|\n",
      "|           drama|       843| 115|\n",
      "|          sultan|       839| 116|\n",
      "|        bangunan|       837| 117|\n",
      "|         telefon|       834| 118|\n",
      "|             bin|       834| 119|\n",
      "|           taman|       832| 120|\n",
      "|            abad|       829| 121|\n",
      "|         manusia|       826| 122|\n",
      "|          anjing|       825| 123|\n",
      "|         bermain|       822| 124|\n",
      "|           perak|       817| 125|\n",
      "|       membentuk|       817| 126|\n",
      "|            cina|       811| 127|\n",
      "|         british|       811| 128|\n",
      "|        kelantan|       801| 129|\n",
      "|        februari|       801| 130|\n",
      "|              ii|       784| 131|\n",
      "|           murid|       782| 132|\n",
      "|              al|       781| 133|\n",
      "|        keluarga|       778| 134|\n",
      "|           itali|       776| 135|\n",
      "|          gambar|       776| 136|\n",
      "|         dicapai|       772| 137|\n",
      "|          eropah|       771| 138|\n",
      "|          sering|       770| 139|\n",
      "|         belanda|       766| 140|\n",
      "|          sangat|       765| 141|\n",
      "|         kembali|       761| 142|\n",
      "|     menyebabkan|       758| 143|\n",
      "|           pekan|       750| 144|\n",
      "|               ü|       741| 145|\n",
      "|     diterbitkan|       733| 146|\n",
      "|          nombor|       731| 147|\n",
      "|          london|       731| 148|\n",
      "|               r|       730| 149|\n",
      "|            jawa|       727| 150|\n",
      "|               k|       725| 151|\n",
      "|         rujukan|       725| 152|\n",
      "|            umum|       725| 153|\n",
      "|             pos|       724| 154|\n",
      "|      perlawanan|       724| 155|\n",
      "|           video|       720| 156|\n",
      "|          united|       717| 157|\n",
      "|    menghasilkan|       717| 158|\n",
      "|           kebal|       715| 159|\n",
      "|           hidup|       707| 160|\n",
      "|            muda|       705| 161|\n",
      "|      ketinggian|       705| 162|\n",
      "|        populasi|       704| 163|\n",
      "|            kaum|       702| 164|\n",
      "|          gunung|       700| 165|\n",
      "|            seni|       698| 166|\n",
      "|      kebanyakan|       696| 167|\n",
      "|               ó|       696| 168|\n",
      "|           udara|       685| 169|\n",
      "|             api|       672| 170|\n",
      "|           mampu|       669| 171|\n",
      "|           cinta|       669| 172|\n",
      "|               l|       664| 173|\n",
      "|          ketiga|       659| 174|\n",
      "|             web|       659| 175|\n",
      "|          kurang|       658| 176|\n",
      "|         membaca|       657| 177|\n",
      "|        penyanyi|       654| 178|\n",
      "|           czech|       653| 179|\n",
      "|       memenangi|       652| 180|\n",
      "|            liga|       648| 181|\n",
      "|         panjang|       646| 182|\n",
      "|         spesies|       645| 183|\n",
      "|          dibina|       645| 184|\n",
      "|    undangundang|       643| 185|\n",
      "|         pesawat|       641| 186|\n",
      "|        keduadua|       639| 187|\n",
      "|           badan|       639| 188|\n",
      "|        bilangan|       637| 189|\n",
      "|        akhirnya|       636| 190|\n",
      "|              km|       634| 191|\n",
      "|           waktu|       632| 192|\n",
      "|          stesen|       631| 193|\n",
      "|            isbn|       627| 194|\n",
      "|      disebabkan|       626| 195|\n",
      "|     dikeluarkan|       626| 196|\n",
      "|          burung|       626| 197|\n",
      "|         berasal|       623| 198|\n",
      "|           johor|       622| 199|\n",
      "|         baginda|       620| 200|\n",
      "|     kemudiannya|       618| 201|\n",
      "|          norway|       618| 202|\n",
      "|            arab|       617| 203|\n",
      "|        dianggap|       617| 204|\n",
      "|          muncul|       617| 205|\n",
      "|            lalu|       616| 206|\n",
      "|            kode|       612| 207|\n",
      "|           watak|       610| 208|\n",
      "|           piala|       609| 209|\n",
      "|              la|       608| 210|\n",
      "|        national|       607| 211|\n",
      "|               í|       606| 212|\n",
      "|             lee|       603| 213|\n",
      "|            haji|       602| 214|\n",
      "|        tenggara|       600| 215|\n",
      "|               ö|       600| 216|\n",
      "|           pokok|       600| 217|\n",
      "|           kelab|       599| 218|\n",
      "|      ringkasnya|       598| 219|\n",
      "|     terutamanya|       595| 220|\n",
      "|               ō|       594| 221|\n",
      "|          padang|       593| 222|\n",
      "|       menentang|       589| 223|\n",
      "|       kejohanan|       589| 224|\n",
      "|           juara|       582| 225|\n",
      "|      televisyen|       582| 226|\n",
      "|      university|       576| 227|\n",
      "|             gol|       576| 228|\n",
      "|        military|       570| 229|\n",
      "|          budaya|       569| 230|\n",
      "|         popular|       569| 231|\n",
      "|          pantai|       569| 232|\n",
      "|           darul|       566| 233|\n",
      "|           insee|       565| 234|\n",
      "|           putih|       561| 235|\n",
      "|             kod|       559| 236|\n",
      "|        terakhir|       556| 237|\n",
      "|       menyertai|       556| 238|\n",
      "|           kelas|       555| 239|\n",
      "|          serbia|       553| 240|\n",
      "|           merah|       552| 241|\n",
      "|            aras|       550| 242|\n",
      "|               ô|       550| 243|\n",
      "|            john|       550| 244|\n",
      "|      ditubuhkan|       549| 245|\n",
      "|         berbeza|       547| 246|\n",
      "|        selangor|       547| 247|\n",
      "|        penyakit|       546| 248|\n",
      "|              st|       546| 249|\n",
      "|               è|       546| 250|\n",
      "|          pinang|       545| 251|\n",
      "|               h|       543| 252|\n",
      "|        aircraft|       542| 253|\n",
      "|       persatuan|       542| 254|\n",
      "|           sains|       542| 255|\n",
      "|        kirakira|       539| 256|\n",
      "|            cara|       538| 257|\n",
      "|     provisional|       538| 258|\n",
      "|      kanakkanak|       537| 259|\n",
      "|               j|       536| 260|\n",
      "|       diberikan|       536| 261|\n",
      "|       reference|       536| 262|\n",
      "|          lokasi|       534| 263|\n",
      "|           world|       534| 264|\n",
      "|       permainan|       530| 265|\n",
      "|    pertandingan|       528| 266|\n",
      "|       memulakan|       527| 267|\n",
      "|       melakukan|       525| 268|\n",
      "|   diterjemahkan|       523| 269|\n",
      "|         istilah|       520| 270|\n",
      "|         berusia|       519| 271|\n",
      "|            kaki|       518| 272|\n",
      "|           warna|       516| 273|\n",
      "|          sekali|       515| 274|\n",
      "|            daya|       514| 275|\n",
      "|               g|       513| 276|\n",
      "|            maka|       510| 277|\n",
      "|    pemerintahan|       509| 278|\n",
      "|            dato|       507| 279|\n",
      "|         masakan|       506| 280|\n",
      "|         welcome|       506| 281|\n",
      "|          hampir|       506| 282|\n",
      "|            nest|       504| 283|\n",
      "|           tanda|       504| 284|\n",
      "|               w|       504| 285|\n",
      "|            jaya|       504| 286|\n",
      "|        muhammad|       503| 287|\n",
      "|              rm|       503| 288|\n",
      "|         bintang|       503| 289|\n",
      "|         peranan|       500| 290|\n",
      "|               f|       499| 291|\n",
      "|         jakarta|       499| 292|\n",
      "|            owls|       498| 293|\n",
      "|      menyatakan|       496| 294|\n",
      "|            hati|       495| 295|\n",
      "|      penggunaan|       494| 296|\n",
      "|       rancangan|       493| 297|\n",
      "|          afrika|       493| 298|\n",
      "|           press|       488| 299|\n",
      "|         merujuk|       483| 300|\n",
      "|          perlis|       483| 301|\n",
      "|             hal|       483| 302|\n",
      "|        belakang|       483| 303|\n",
      "|           hitam|       480| 304|\n",
      "|            asli|       480| 305|\n",
      "|           david|       480| 306|\n",
      "|           karya|       475| 307|\n",
      "|         tinggal|       474| 308|\n",
      "|          dibuat|       474| 309|\n",
      "|      sesetengah|       473| 310|\n",
      "|          laluan|       472| 311|\n",
      "|          istana|       471| 312|\n",
      "|            aceh|       469| 313|\n",
      "|          diberi|       468| 314|\n",
      "|          melaka|       468| 315|\n",
      "|       dilakukan|       467| 316|\n",
      "|          arahan|       467| 317|\n",
      "|          pahang|       466| 318|\n",
      "|         bekerja|       465| 319|\n",
      "|        sembilan|       465| 320|\n",
      "|          haiwan|       464| 321|\n",
      "|         penulis|       463| 322|\n",
      "|        komuniti|       462| 323|\n",
      "|            mati|       461| 324|\n",
      "|              sm|       461| 325|\n",
      "|          berita|       460| 326|\n",
      "|            bumi|       460| 327|\n",
      "|        serangan|       459| 328|\n",
      "|       bandaraya|       459| 329|\n",
      "|         kampong|       457| 330|\n",
      "|            arah|       456| 331|\n",
      "|        dilantik|       456| 332|\n",
      "|             jam|       455| 333|\n",
      "|            ikan|       454| 334|\n",
      "|        tambahan|       451| 335|\n",
      "|     mengandungi|       450| 336|\n",
      "|        kematian|       449| 337|\n",
      "|          akibat|       449| 338|\n",
      "|               v|       449| 339|\n",
      "|            khas|       448| 340|\n",
      "|           rusia|       447| 341|\n",
      "|         senjata|       446| 342|\n",
      "|           panas|       446| 343|\n",
      "|          gereja|       445| 344|\n",
      "|          single|       445| 345|\n",
      "|           walau|       444| 346|\n",
      "|            shah|       444| 347|\n",
      "|             hak|       442| 348|\n",
      "|          begitu|       442| 349|\n",
      "|            tuan|       438| 350|\n",
      "|   international|       438| 351|\n",
      "|       meninggal|       437| 352|\n",
      "|           moden|       436| 353|\n",
      "|          darjah|       434| 354|\n",
      "|          yahudi|       434| 355|\n",
      "|       retrieved|       434| 356|\n",
      "|          diraja|       433| 357|\n",
      "|           model|       433| 358|\n",
      "|           tetap|       433| 359|\n",
      "|          daging|       432| 360|\n",
      "|      dilahirkan|       429| 361|\n",
      "|          mexico|       428| 362|\n",
      "|          purata|       428| 363|\n",
      "|          meriam|       427| 364|\n",
      "|           mesir|       427| 365|\n",
      "|      dihasilkan|       427| 366|\n",
      "|               x|       426| 367|\n",
      "|           wujud|       422| 368|\n",
      "|         gelaran|       421| 369|\n",
      "|        langsung|       419| 370|\n",
      "|           bebas|       418| 371|\n",
      "|          kajian|       416| 372|\n",
      "|           versi|       412| 373|\n",
      "|         barisan|       411| 374|\n",
      "|          tangan|       406| 375|\n",
      "|          putera|       405| 376|\n",
      "|               ā|       405| 377|\n",
      "|          ismail|       404| 378|\n",
      "|         empayar|       404| 379|\n",
      "|           nepal|       403| 380|\n",
      "|      pemerintah|       403| 381|\n",
      "|        melebihi|       403| 382|\n",
      "|     bersempadan|       402| 383|\n",
      "|    meninggalkan|       402| 384|\n",
      "|       dinamakan|       402| 385|\n",
      "|        penerbit|       401| 386|\n",
      "|       dipanggil|       401| 387|\n",
      "|       perkataan|       400| 388|\n",
      "|      seterusnya|       399| 389|\n",
      "|              le|       399| 390|\n",
      "|         sejenis|       398| 391|\n",
      "|              sk|       397| 392|\n",
      "|        pusingan|       394| 393|\n",
      "|          kanada|       394| 394|\n",
      "|     persembahan|       393| 395|\n",
      "|         disebut|       392| 396|\n",
      "|            siti|       391| 397|\n",
      "|           pasir|       390| 398|\n",
      "|            luas|       390| 399|\n",
      "|         sebelah|       387| 400|\n",
      "|           allah|       386| 401|\n",
      "|          sosial|       386| 402|\n",
      "|      berkhidmat|       386| 403|\n",
      "|            kuat|       385| 404|\n",
      "|           bilik|       385| 405|\n",
      "|       kehidupan|       384| 406|\n",
      "|     ditayangkan|       384| 407|\n",
      "|          tarikh|       384| 408|\n",
      "|           kanan|       384| 409|\n",
      "|            peta|       383| 410|\n",
      "|        tertentu|       383| 411|\n",
      "|            saat|       383| 412|\n",
      "|     pertempuran|       381| 413|\n",
      "|        angkatan|       381| 414|\n",
      "|          ladang|       381| 415|\n",
      "|         england|       380| 416|\n",
      "|            york|       380| 417|\n",
      "|           masuk|       379| 418|\n",
      "|         mencari|       379| 419|\n",
      "|           penuh|       377| 420|\n",
      "|         berikut|       377| 421|\n",
      "|            undi|       376| 422|\n",
      "|        kesatuan|       376| 423|\n",
      "|             abu|       375| 424|\n",
      "|         ibrahim|       375| 425|\n",
      "|             rom|       375| 426|\n",
      "|           hutan|       373| 427|\n",
      "|       tertinggi|       371| 428|\n",
      "|           tapak|       370| 429|\n",
      "|    negaranegara|       369| 430|\n",
      "|               ı|       369| 431|\n",
      "|    perpustakaan|       368| 432|\n",
      "|        terlibat|       367| 433|\n",
      "|       mengalami|       365| 434|\n",
      "|         ditulis|       364| 435|\n",
      "|         website|       364| 436|\n",
      "|          contoh|       363| 437|\n",
      "|            padi|       363| 438|\n",
      "|        komputer|       363| 439|\n",
      "|            city|       363| 440|\n",
      "|            nabi|       362| 441|\n",
      "|          keluar|       362| 442|\n",
      "|           ulang|       362| 443|\n",
      "|           surat|       361| 444|\n",
      "|           darat|       361| 445|\n",
      "|          portal|       360| 446|\n",
      "|               š|       360| 447|\n",
      "|         separuh|       359| 448|\n",
      "|        sempadan|       359| 449|\n",
      "|         bersatu|       359| 450|\n",
      "|         tradisi|       359| 451|\n",
      "|         saluran|       358| 452|\n",
      "|         sedikit|       357| 453|\n",
      "|        official|       356| 454|\n",
      "|           lembu|       356| 455|\n",
      "|         diambil|       355| 456|\n",
      "|    menggantikan|       355| 457|\n",
      "|          israel|       355| 458|\n",
      "|         susunan|       355| 459|\n",
      "|            kayu|       355| 460|\n",
      "|       perubahan|       354| 461|\n",
      "|           lihat|       353| 462|\n",
      "|           minit|       353| 463|\n",
      "|       pertanian|       352| 464|\n",
      "|     berhampiran|       351| 465|\n",
      "|           suara|       351| 466|\n",
      "|         dipilih|       350| 467|\n",
      "|        memegang|       349| 468|\n",
      "|            park|       348| 469|\n",
      "|       kebolehan|       347| 470|\n",
      "|         kingdom|       346| 471|\n",
      "|            buat|       346| 472|\n",
      "|       takahashi|       346| 473|\n",
      "|            jadi|       345| 474|\n",
      "|         history|       345| 475|\n",
      "|            bapa|       345| 476|\n",
      "|           radio|       345| 477|\n",
      "|    mengeluarkan|       344| 478|\n",
      "|         menulis|       344| 479|\n",
      "|      pertahanan|       343| 480|\n",
      "|       perubatan|       343| 481|\n",
      "|            kiri|       343| 482|\n",
      "|             ali|       343| 483|\n",
      "|           kulit|       342| 484|\n",
      "|           agung|       342| 485|\n",
      "|      melibatkan|       341| 486|\n",
      "|        kategori|       340| 487|\n",
      "|            agar|       340| 488|\n",
      "|            hong|       340| 489|\n",
      "|           hujan|       340| 490|\n",
      "|          hadiah|       339| 491|\n",
      "|        filipina|       338| 492|\n",
      "|            jauh|       337| 493|\n",
      "|          kepala|       337| 494|\n",
      "|           kesan|       335| 495|\n",
      "|          bangsa|       334| 496|\n",
      "|         komunis|       334| 497|\n",
      "|          tujuan|       334| 498|\n",
      "|       rangkaian|       334| 499|\n",
      "|         vietnam|       333| 500|\n",
      "|     bersamasama|       333| 501|\n",
      "|         sesuatu|       333| 502|\n",
      "|        kristian|       332| 503|\n",
      "|        hidangan|       332| 504|\n",
      "|       pelajaran|       332| 505|\n",
      "|       kepulauan|       332| 506|\n",
      "|          puteri|       331| 507|\n",
      "|               č|       331| 508|\n",
      "|       diketahui|       331| 509|\n",
      "|           berat|       331| 510|\n",
      "|          cerita|       330| 511|\n",
      "|            alat|       330| 512|\n",
      "|               ú|       328| 513|\n",
      "|          rahman|       326| 514|\n",
      "|       penubuhan|       326| 515|\n",
      "|          episod|       326| 516|\n",
      "|         bantuan|       325| 517|\n",
      "|        gabungan|       324| 518|\n",
      "|         dirinya|       324| 519|\n",
      "|          dahulu|       323| 520|\n",
      "|         membina|       322| 521|\n",
      "|        autonomi|       322| 522|\n",
      "|           edisi|       321| 523|\n",
      "|           artis|       321| 524|\n",
      "|            kong|       320| 525|\n",
      "|      orangorang|       319| 526|\n",
      "|          aliran|       318| 527|\n",
      "|              th|       317| 528|\n",
      "|        berkuasa|       317| 529|\n",
      "|     pertengahan|       316| 530|\n",
      "|             jpg|       315| 531|\n",
      "|       disiarkan|       314| 532|\n",
      "|        majoriti|       314| 533|\n",
      "|      penampilan|       313| 534|\n",
      "|         asalnya|       313| 535|\n",
      "|       permukaan|       312| 536|\n",
      "|        original|       311| 537|\n",
      "|            aziz|       311| 538|\n",
      "|         dilihat|       311| 539|\n",
      "|            rasa|       310| 540|\n",
      "|       pengajian|       310| 541|\n",
      "|           jarak|       310| 542|\n",
      "|          george|       308| 543|\n",
      "|    perlembagaan|       308| 544|\n",
      "|          kerusi|       307| 545|\n",
      "|     dilancarkan|       306| 546|\n",
      "|        terbitan|       305| 547|\n",
      "|          studio|       304| 548|\n",
      "|          yunani|       304| 549|\n",
      "|          brunei|       302| 550|\n",
      "|           bunga|       302| 551|\n",
      "|           etnik|       302| 552|\n",
      "|      pergerakan|       301| 553|\n",
      "|            cuba|       299| 554|\n",
      "|         latitud|       299| 555|\n",
      "|       dikatakan|       299| 556|\n",
      "|         kawalan|       298| 557|\n",
      "|      organisasi|       298| 558|\n",
      "|         gerakan|       298| 559|\n",
      "|     dibahagikan|       297| 560|\n",
      "|           mulai|       297| 561|\n",
      "|           enjin|       295| 562|\n",
      "|         dinasti|       295| 563|\n",
      "|       including|       295| 564|\n",
      "|              si|       294| 565|\n",
      "|       berpindah|       293| 566|\n",
      "|           aktif|       292| 567|\n",
      "|      commission|       292| 568|\n",
      "|         jeneral|       291| 569|\n",
      "|         kerjaya|       290| 570|\n",
      "|         romania|       289| 571|\n",
      "|      demokratik|       289| 572|\n",
      "|            ilmu|       289| 573|\n",
      "|         tanjung|       289| 574|\n",
      "|      sepenuhnya|       289| 575|\n",
      "|        didapati|       289| 576|\n",
      "|            news|       288| 577|\n",
      "|              pp|       288| 578|\n",
      "|          soviet|       287| 579|\n",
      "|          peluru|       287| 580|\n",
      "|            aman|       287| 581|\n",
      "|          cities|       286| 582|\n",
      "|           silat|       286| 583|\n",
      "|          sesuai|       285| 584|\n",
      "|       antaranya|       285| 585|\n",
      "|              mm|       285| 586|\n",
      "|       bermaksud|       284| 587|\n",
      "|             iii|       284| 588|\n",
      "|           bunyi|       283| 589|\n",
      "|         latihan|       283| 590|\n",
      "|            emas|       283| 591|\n",
      "|        elektrik|       283| 592|\n",
      "|           ruang|       283| 593|\n",
      "|        maharaja|       282| 594|\n",
      "|     tradisional|       282| 595|\n",
      "|          harian|       282| 596|\n",
      "|     semenanjung|       282| 597|\n",
      "|           tuhan|       282| 598|\n",
      "|      pertamanya|       282| 599|\n",
      "|         majalah|       282| 600|\n",
      "|        undangan|       281| 601|\n",
      "|        membunuh|       281| 602|\n",
      "|      berkembang|       280| 603|\n",
      "|       kilometer|       280| 604|\n",
      "|             pun|       280| 605|\n",
      "|           sebab|       279| 606|\n",
      "|        archived|       279| 607|\n",
      "|           novel|       279| 608|\n",
      "|             era|       279| 609|\n",
      "|            love|       279| 610|\n",
      "|    penyelidikan|       279| 611|\n",
      "|         rencana|       278| 612|\n",
      "|            agak|       278| 613|\n",
      "|            usia|       277| 614|\n",
      "|          pingat|       277| 615|\n",
      "|          khusus|       277| 616|\n",
      "|           tujuh|       277| 617|\n",
      "|          konsep|       277| 618|\n",
      "|           layak|       276| 619|\n",
      "|        memasuki|       276| 620|\n",
      "|           teluk|       275| 621|\n",
      "|       pengeluar|       275| 622|\n",
      "|        individu|       274| 623|\n",
      "|          brazil|       274| 624|\n",
      "|         rakaman|       273| 625|\n",
      "|       kenderaan|       273| 626|\n",
      "|           towns|       273| 627|\n",
      "|       seseorang|       273| 628|\n",
      "|      bertanding|       273| 629|\n",
      "|        tumbuhan|       273| 630|\n",
      "|            iman|       272| 631|\n",
      "|          hassan|       272| 632|\n",
      "|         puchong|       272| 633|\n",
      "|        keluasan|       272| 634|\n",
      "|           cukup|       272| 635|\n",
      "|        hospital|       271| 636|\n",
      "|        longitud|       271| 637|\n",
      "|        villages|       270| 638|\n",
      "|      bergantung|       270| 639|\n",
      "|       berkahwin|       269| 640|\n",
      "|           kedai|       269| 641|\n",
      "|    munisipaliti|       269| 642|\n",
      "|          martin|       269| 643|\n",
      "|         diarkib|       268| 644|\n",
      "|         keempat|       268| 645|\n",
      "|             tak|       268| 646|\n",
      "|         william|       267| 647|\n",
      "|          swasta|       267| 648|\n",
      "|          pendek|       266| 649|\n",
      "|        terpaksa|       266| 650|\n",
      "|      sebenarnya|       266| 651|\n",
      "|         konsert|       266| 652|\n",
      "|          online|       266| 653|\n",
      "|         kongres|       265| 654|\n",
      "|        kejayaan|       265| 655|\n",
      "|      dibintangi|       265| 656|\n",
      "|           balas|       264| 657|\n",
      "|           james|       264| 658|\n",
      "|            suhu|       263| 659|\n",
      "|        mewakili|       261| 660|\n",
      "|      perjalanan|       261| 661|\n",
      "|             kim|       261| 662|\n",
      "|      beroperasi|       261| 663|\n",
      "|         kesemua|       261| 664|\n",
      "|           right|       260| 665|\n",
      "|        struktur|       260| 666|\n",
      "|        latitude|       260| 667|\n",
      "|       dijadikan|       258| 668|\n",
      "|           hijau|       258| 669|\n",
      "|             wan|       258| 670|\n",
      "|           jatuh|       257| 671|\n",
      "|            gaya|       255| 672|\n",
      "|      memerlukan|       255| 673|\n",
      "|          menara|       255| 674|\n",
      "|      perlumbaan|       254| 675|\n",
      "|         ataupun|       254| 676|\n",
      "|           south|       254| 677|\n",
      "|      kalimantan|       254| 678|\n",
      "|        kegunaan|       253| 679|\n",
      "|        bergerak|       253| 680|\n",
      "|         alquran|       253| 681|\n",
      "|           calon|       253| 682|\n",
      "|           askar|       253| 683|\n",
      "|          street|       252| 684|\n",
      "|           kimia|       252| 685|\n",
      "|         lembaga|       252| 686|\n",
      "|         olimpik|       252| 687|\n",
      "|          remaja|       252| 688|\n",
      "|           milik|       252| 689|\n",
      "|            fifa|       251| 690|\n",
      "|          muzium|       251| 691|\n",
      "|           darah|       251| 692|\n",
      "|      penerbitan|       251| 693|\n",
      "|      longtitude|       250| 694|\n",
      "|      pembunuhan|       250| 695|\n",
      "|        pemenang|       250| 696|\n",
      "|           hotel|       250| 697|\n",
      "|             aku|       250| 698|\n",
      "|          kaedah|       248| 699|\n",
      "|            rock|       248| 700|\n",
      "|            star|       248| 701|\n",
      "|         lakonan|       247| 702|\n",
      "|     kemerdekaan|       247| 703|\n",
      "|            road|       247| 704|\n",
      "|           lapan|       247| 705|\n",
      "|          poland|       246| 706|\n",
      "|           rekod|       246| 707|\n",
      "|             smk|       246| 708|\n",
      "|          status|       246| 709|\n",
      "|          sydney|       245| 710|\n",
      "|           harus|       245| 711|\n",
      "|           music|       245| 712|\n",
      "|        diterima|       245| 713|\n",
      "|            maju|       245| 714|\n",
      "|           grand|       243| 715|\n",
      "|           lebuh|       243| 716|\n",
      "|       peristiwa|       243| 717|\n",
      "|            nasi|       242| 718|\n",
      "|         akademi|       242| 719|\n",
      "|    masingmasing|       242| 720|\n",
      "|         belajar|       242| 721|\n",
      "|          cahaya|       242| 722|\n",
      "|            fail|       242| 723|\n",
      "|           kisah|       242| 724|\n",
      "|           ingin|       242| 725|\n",
      "|          sampai|       242| 726|\n",
      "|           pergi|       241| 727|\n",
      "|       menyokong|       241| 728|\n",
      "|        meraikan|       241| 729|\n",
      "|               ç|       241| 730|\n",
      "|      sebaliknya|       240| 731|\n",
      "|         mahupun|       240| 732|\n",
      "|        generasi|       240| 733|\n",
      "|         ditemui|       239| 734|\n",
      "|             pbb|       239| 735|\n",
      "|          lembah|       239| 736|\n",
      "|           jelas|       238| 737|\n",
      "|     profesional|       238| 738|\n",
      "|         angkasa|       237| 739|\n",
      "|           paris|       237| 740|\n",
      "|     information|       237| 741|\n",
      "|            daun|       237| 742|\n",
      "|               ä|       237| 743|\n",
      "|              fc|       236| 744|\n",
      "|              el|       236| 745|\n",
      "|        pengaruh|       236| 746|\n",
      "|            ayam|       236| 747|\n",
      "|             san|       235| 748|\n",
      "|           ujian|       235| 749|\n",
      "|          muslim|       234| 750|\n",
      "|           gagal|       234| 751|\n",
      "|        golongan|       233| 752|\n",
      "|           kasih|       233| 753|\n",
      "|           sejuk|       233| 754|\n",
      "|         tingkat|       233| 755|\n",
      "|       bertindak|       233| 756|\n",
      "|         memilih|       232| 757|\n",
      "|        panglima|       232| 758|\n",
      "|           gajah|       232| 759|\n",
      "|           kolej|       232| 760|\n",
      "|            reka|       232| 761|\n",
      "|         sewaktu|       232| 762|\n",
      "|        institut|       231| 763|\n",
      "|          isteri|       231| 764|\n",
      "|         saudara|       231| 765|\n",
      "|            page|       231| 766|\n",
      "|      kemenangan|       230| 767|\n",
      "|      merangkumi|       230| 768|\n",
      "|    perkembangan|       230| 769|\n",
      "|          school|       230| 770|\n",
      "|           makan|       229| 771|\n",
      "|     switzerland|       229| 772|\n",
      "|        lainlain|       229| 773|\n",
      "|     perhimpunan|       229| 774|\n",
      "|             mas|       229| 775|\n",
      "|      perkuburan|       229| 776|\n",
      "|        manamana|       228| 777|\n",
      "|         tunggal|       228| 778|\n",
      "|         dicipta|       228| 779|\n",
      "|       keturunan|       227| 780|\n",
      "|            naib|       227| 781|\n",
      "|     berpenduduk|       227| 782|\n",
      "|           parsi|       227| 783|\n",
      "|        kompleks|       226| 784|\n",
      "|         koleksi|       226| 785|\n",
      "|         sempena|       226| 786|\n",
      "|       kelayakan|       226| 787|\n",
      "|         youtube|       226| 788|\n",
      "|          ringan|       226| 789|\n",
      "|           nobel|       226| 790|\n",
      "|            baka|       226| 791|\n",
      "|           times|       225| 792|\n",
      "|         stadium|       225| 793|\n",
      "|             man|       225| 794|\n",
      "|       menyerang|       225| 795|\n",
      "|     mengekalkan|       224| 796|\n",
      "|           belas|       224| 797|\n",
      "|        terutama|       224| 798|\n",
      "|           tasik|       224| 799|\n",
      "|        kejadian|       224| 800|\n",
      "|          lautan|       223| 801|\n",
      "|         persegi|       223| 802|\n",
      "|            khan|       223| 803|\n",
      "|           malah|       223| 804|\n",
      "|          kuning|       222| 805|\n",
      "|            film|       222| 806|\n",
      "|          menang|       222| 807|\n",
      "|           dekat|       222| 808|\n",
      "|       peraturan|       222| 809|\n",
      "|        american|       222| 810|\n",
      "|      philippine|       222| 811|\n",
      "|     berlangsung|       222| 812|\n",
      "|       contohnya|       221| 813|\n",
      "|          system|       221| 814|\n",
      "|           vokal|       221| 815|\n",
      "|      kehilangan|       220| 816|\n",
      "|             sel|       220| 817|\n",
      "|          fungsi|       219| 818|\n",
      "|      kebudayaan|       219| 819|\n",
      "|           bharu|       219| 820|\n",
      "|        ahliahli|       219| 821|\n",
      "|         digital|       219| 822|\n",
      "|          dibawa|       219| 823|\n",
      "|     menampilkan|       219| 824|\n",
      "|          thomas|       218| 825|\n",
      "|      mengatakan|       218| 826|\n",
      "|         michael|       218| 827|\n",
      "|          direka|       217| 828|\n",
      "|            mara|       217| 829|\n",
      "|   entertainment|       217| 830|\n",
      "|             one|       217| 831|\n",
      "|           puluh|       216| 832|\n",
      "|        festival|       216| 833|\n",
      "|         ireland|       216| 834|\n",
      "|            sana|       215| 835|\n",
      "|            bulu|       215| 836|\n",
      "|        berwarna|       215| 837|\n",
      "|              jl|       215| 838|\n",
      "|           pasti|       215| 839|\n",
      "|         mulanya|       215| 840|\n",
      "|           house|       214| 841|\n",
      "|        sejumlah|       214| 842|\n",
      "|             may|       214| 843|\n",
      "|               z|       214| 844|\n",
      "|           astro|       213| 845|\n",
      "|           setia|       213| 846|\n",
      "|             der|       213| 847|\n",
      "|           teori|       213| 848|\n",
      "|         hungary|       213| 849|\n",
      "|           tokoh|       213| 850|\n",
      "|        meskipun|       212| 851|\n",
      "|           angin|       212| 852|\n",
      "|        standard|       211| 853|\n",
      "|        internet|       211| 854|\n",
      "|             pas|       211| 855|\n",
      "|     melancarkan|       211| 856|\n",
      "|            alih|       211| 857|\n",
      "|            paus|       210| 858|\n",
      "|            hulu|       210| 859|\n",
      "|          dijual|       210| 860|\n",
      "|         charles|       210| 861|\n",
      "|        pakistan|       210| 862|\n",
      "|            misi|       209| 863|\n",
      "|       sekiranya|       209| 864|\n",
      "|           manis|       209| 865|\n",
      "|            best|       209| 866|\n",
      "|       institusi|       209| 867|\n",
      "|            muka|       209| 868|\n",
      "|               ž|       209| 869|\n",
      "|            blok|       209| 870|\n",
      "|           north|       208| 871|\n",
      "|              et|       208| 872|\n",
      "|       pelabuhan|       208| 873|\n",
      "|    pengangkutan|       208| 874|\n",
      "|             red|       208| 875|\n",
      "|          terhad|       208| 876|\n",
      "|       diumumkan|       207| 877|\n",
      "|           wakil|       207| 878|\n",
      "|      berasaskan|       207| 879|\n",
      "|      menubuhkan|       207| 880|\n",
      "|            biru|       207| 881|\n",
      "|         sebenar|       206| 882|\n",
      "|        jambatan|       206| 883|\n",
      "|    kepelbagaian|       206| 884|\n",
      "|        penonton|       206| 885|\n",
      "|          robert|       206| 886|\n",
      "|          malaya|       206| 887|\n",
      "|       memainkan|       206| 888|\n",
      "|            tiba|       206| 889|\n",
      "|           putra|       206| 890|\n",
      "|            miss|       205| 891|\n",
      "|          meluas|       205| 892|\n",
      "|      memperoleh|       205| 893|\n",
      "|          planet|       205| 894|\n",
      "|          utusan|       205| 895|\n",
      "|             teh|       205| 896|\n",
      "|        pasangan|       205| 897|\n",
      "|               ş|       205| 898|\n",
      "|         diikuti|       204| 899|\n",
      "|            alor|       204| 900|\n",
      "|         zealand|       204| 901|\n",
      "|       perhatian|       203| 902|\n",
      "|         tulisan|       203| 903|\n",
      "|           tamil|       203| 904|\n",
      "|         samping|       203| 905|\n",
      "|      menentukan|       203| 906|\n",
      "|           medan|       202| 907|\n",
      "|         katolik|       202| 908|\n",
      "|           gadis|       202| 909|\n",
      "|      diperlukan|       202| 910|\n",
      "|         jajahan|       201| 911|\n",
      "|     termasuklah|       201| 912|\n",
      "|          hujung|       200| 913|\n",
      "|              ng|       200| 914|\n",
      "|           telur|       200| 915|\n",
      "|        matahari|       200| 916|\n",
      "|     satusatunya|       200| 917|\n",
      "|        kalendar|       200| 918|\n",
      "|           keras|       200| 919|\n",
      "|              uk|       200| 920|\n",
      "|      memutuskan|       200| 921|\n",
      "|          kempen|       199| 922|\n",
      "|         pemandu|       199| 923|\n",
      "|         gabenor|       199| 924|\n",
      "|       diarahkan|       199| 925|\n",
      "|           bakar|       199| 926|\n",
      "|          kacang|       199| 927|\n",
      "|           balai|       199| 928|\n",
      "|            euro|       199| 929|\n",
      "|              ne|       199| 930|\n",
      "|         bertemu|       198| 931|\n",
      "|           musuh|       198| 932|\n",
      "|      berdekatan|       198| 933|\n",
      "|          wesley|       198| 934|\n",
      "|       berbentuk|       197| 935|\n",
      "|         sastera|       197| 936|\n",
      "|             pop|       197| 937|\n",
      "|            omar|       197| 938|\n",
      "|              na|       197| 939|\n",
      "|           tokyo|       197| 940|\n",
      "|             van|       197| 941|\n",
      "|      berikutnya|       197| 942|\n",
      "|bertanggungjawab|       197| 943|\n",
      "|         pangkat|       196| 944|\n",
      "|         dimakan|       196| 945|\n",
      "|           titik|       196| 946|\n",
      "|              lt|       196| 947|\n",
      "|           warga|       196| 948|\n",
      "|             zon|       196| 949|\n",
      "|      menghantar|       195| 950|\n",
      "|          amalan|       195| 951|\n",
      "|          teknik|       194| 952|\n",
      "|           huruf|       194| 953|\n",
      "|           unsur|       194| 954|\n",
      "|     dibangunkan|       194| 955|\n",
      "|        sulawesi|       194| 956|\n",
      "|          centre|       194| 957|\n",
      "|            maya|       194| 958|\n",
      "|        sumatera|       193| 959|\n",
      "|          serupa|       193| 960|\n",
      "|            akta|       193| 961|\n",
      "|        dijumpai|       193| 962|\n",
      "|          ibunya|       192| 963|\n",
      "|             war|       192| 964|\n",
      "|       menduduki|       192| 965|\n",
      "|           opera|       192| 966|\n",
      "|          oxford|       192| 967|\n",
      "|              ed|       192| 968|\n",
      "|           gitar|       192| 969|\n",
      "|       mendapati|       192| 970|\n",
      "|           first|       192| 971|\n",
      "|         animasi|       191| 972|\n",
      "|            saiz|       191| 973|\n",
      "|          komedi|       191| 974|\n",
      "|           tiram|       191| 975|\n",
      "|            demi|       191| 976|\n",
      "|       bertujuan|       191| 977|\n",
      "|         tekanan|       190| 978|\n",
      "|       permulaan|       190| 979|\n",
      "|           garis|       190| 980|\n",
      "|     membenarkan|       190| 981|\n",
      "|            site|       190| 982|\n",
      "|        dihantar|       189| 983|\n",
      "|              en|       189| 984|\n",
      "|   diperkenalkan|       189| 985|\n",
      "|       khususnya|       189| 986|\n",
      "|    publications|       189| 987|\n",
      "|           group|       189| 988|\n",
      "|           kerap|       189| 989|\n",
      "|    kejuruteraan|       188| 990|\n",
      "|    melaksanakan|       188| 991|\n",
      "|         kecuali|       188| 992|\n",
      "|           duduk|       188| 993|\n",
      "|           hantu|       188| 994|\n",
      "|      digantikan|       188| 995|\n",
      "|          dimana|       188| 996|\n",
      "|        geografi|       188| 997|\n",
      "|       kelurahan|       188| 998|\n",
      "|         setahun|       188| 999|\n",
      "|       kelihatan|       187|1000|\n",
      "+----------------+----------+----+\n",
      "only showing top 1000 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter keyword :  kampung\n",
      "Again? (y / n) :   n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your selected keys is ['kampung']\n",
      "================ SAVE THE FILTERED DATA ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========SENTENCE WITH WORD COUNT==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...(0 + 20) / 20]\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date![nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date![nltk_data]   Package stopwords is already up-to-date!\n",
      "\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using state  server backend.\n",
      "Using state  server backend.\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Using state  server backend.\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Using state  server backend.\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Using state  server backend.\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Using state  server backend.\n",
      "Using state  server backend.\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Using state  server backend.\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Using state  server backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.485 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.486 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.485 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.491 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.493 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.485 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.488 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.494 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.492 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.483 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.498 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.508 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.498 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.500 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.502 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.494 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.503 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.492 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.499 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://10.123.51.78:8020/user/wikipedia/filtered_bm_data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========SENTENCE WITHOUT OUTLIERS==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://10.123.51.78:8020/user/wikipedia/bm_sentence_with_statistics\n",
      "22/10/10 21:34:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/10 21:34:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/10 21:34:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/10 21:34:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/10 21:34:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write 0\n",
      "LEN = 872\n",
      "write 1\n",
      "LEN = 873\n",
      "write 2\n",
      "LEN = 873\n",
      "write 3\n",
      "LEN = 873\n",
      "the file is splitted into 4 files to ease the prcessing\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sekolah menengah kebangsaan agama dato haji abbas atau nama ringkasnya smkadha ialah sebuah sekolah menengah agama yang terletak di kampung tok jiring kuala nerus malaysia                    |\n",
      "|loengbygda merupakan sebuah kampung yang terletak di dalam negara norway                                                                                                                       |\n",
      "|lundagrendi merupakan sebuah kampung yang terletak di dalam negara norway                                                                                                                      |\n",
      "|sebahagian jalan ke pekan batu hulu langat telah ditenggelamkan dan perkampungan orang asli temuan kampung donglai telah dipindahkan ke lokasi baru iaitu perkampungan orang asli sungai lalang|\n",
      "|sungai batangsi terletak di belakang kampung pasir dan panjangnya km juga                                                                                                                      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 21:34:52.870957: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 21:34:53.521636: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:34:53.521665: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-10 21:34:53.544389: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-10 21:34:55.884204: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:34:55.884271: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:34:55.884281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya_boilerplate/frozen_graph.py:36: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  f'Cannot import beam_search_ops from Tensorflow Addons, {USED_TREE} will not available to use, make sure Tensorflow Addons version >= 0.12.0'\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya_boilerplate/frozen_graph.py:39: UserWarning: check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases\n",
      "  'check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases')\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3361\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3879\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "2022-10-10 21:35:02.763322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 21:35:02.765038: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:35:02.765068: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-10 21:35:02.765091: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (g2.bigtop.it): /proc/driver/nvidia/version does not exist\n",
      "2022-10-10 21:35:03.476577: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_27705/4115463798.py\", line 216, in <module>\n",
      "    write_avro(tag, f\"OUT/{i}\")\n",
      "  File \"/home/pc/Assignment/Wikipedia/Main/Program/FileIO.py\", line 40, in write_avro\n",
      "    return df.write.mode(\"overwrite\").format(\"avro\").save(path, header = 'true')\n",
      "  File \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/readwriter.py\", line 968, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1320, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/pc/anaconda3/envs/conda_env/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27705/4115463798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tag\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposUDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mwrite_avro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"OUT/{i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclearCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Assignment/Wikipedia/Main/Program/FileIO.py\u001b[0m in \u001b[0;36mwrite_avro\u001b[0;34m(df, path)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_avro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n",
      "\u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2102\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "## arguments\n",
    "pyspark_bin_path = \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3\"\n",
    "python_path = \"/home/pc/anaconda3/envs/conda_env/bin/python3.7\"\n",
    "app_name = \"social media\"\n",
    "data_path = \"/home/pc/data/parsed_data/2parsed-extract-mswiki.txt\"\n",
    "hdfs_working_path = \"hdfs://10.123.51.78:8020/user/wikipedia/\"\n",
    "malay_stopword_path = \"/home/pc/Assignment/node_modules/stopwords-ms/stopwords-ms.json\"\n",
    "jieba_replace_csv = \"/home/pc/Assignment/SocialMedia/Main/replacement.csv\"\n",
    "hdfsHost = \"hdfs://10.123.51.78\"\n",
    "hdfsPort = \"8020\"\n",
    "hdfs_working_path_file = \"/user/wikipedia/\"\n",
    "hiveWarehouseDirectory = \"hdfs://10.123.51.78:8020/user/hive/warehouse\"\n",
    "hiveTriftServerAddress = \"thrift://g2.bigtop.it:9083\"\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import findspark\n",
    "findspark.init(pyspark_bin_path)\n",
    "from FileIO import *\n",
    "import utilities as u\n",
    "import pyspark\n",
    "from Preprocessing import *\n",
    "from pyspark.sql import SparkSession\n",
    "from DetectLanguage import *\n",
    "from LangAgg import *\n",
    "from WordCount import *\n",
    "from pyspark.sql.window import Window\n",
    "from nltk.corpus import stopwords\n",
    "from FilterOutliers import *\n",
    "from postagChinese import *\n",
    "from JiebaTagToStandardTag import *\n",
    "from SegmentDataframe import *\n",
    "from Alternator import *\n",
    "from TokenLanguageReplaceTool import *\n",
    "from pyspark.ml.feature import NGram\n",
    "from KeyGram import *\n",
    "from MiddleKey import *\n",
    "from StopwordsFilteringTool import *\n",
    "from SeqID import *\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "## initialize spark session\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"hdfs\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"70g\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", hiveWarehouseDirectory) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .config(\"hive.metastore.uris\", hiveTriftServerAddress) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "## convert data into avro format\n",
    "bm = spark.read.text(data_path).sample(0.05)\n",
    "print(bm.count())\n",
    "print(\"================ CONVERTING DATA TO AVRO FORMAT ================\")\n",
    "write_avro(bm, f\"{hdfs_working_path}oriDataBM\")\n",
    "################################################################################################################\n",
    "## preprocess data that will affect sentence tokenization\n",
    "df = read_avro(spark, f\"{hdfs_working_path}oriDataBM\")\n",
    "pp = pp(df)\n",
    "\n",
    "print(\"================ SAVE THE PREPROCESSED DATA ================\")\n",
    "write_avro(pp, f\"{hdfs_working_path}ppDataBM\")\n",
    "remove_from_hdfs(f\"{hdfs_working_path}oriDataBM\")\n",
    "# ################################################################################################################\n",
    "# ## doing sentence tokenization on the dataset\n",
    "df = read_avro(spark, f\"{hdfs_working_path}ppDataBM\")\n",
    "bmSent = get_eng_senDF(df)\n",
    "bmSent= bmSent.withColumn(\"text\", explode(bmSent.columns[0])).select(\"text\")\n",
    "\n",
    "print(\"================ SAVE THE SENTENCE TOKENIZE DATA ================\")\n",
    "write_avro(bmSent, hdfs_working_path+\"sent_tokenize_bm.avro\")\n",
    "remove_from_hdfs(f\"{hdfs_working_path}ppDataBM\")\n",
    "# ################################################################################################################\n",
    "# ## preprocessing II\n",
    "df = read_avro(spark, f\"{hdfs_working_path}sent_tokenize_bm.avro\")\n",
    "p2 = preprocessing2(df, \"text\")\n",
    "\n",
    "print(\"================ SAVE THE CLEANED SENTENCE DATA ================\")\n",
    "write_avro(p2, hdfs_working_path+\"p2_bm\")\n",
    "remove_from_hdfs(f\"{hdfs_working_path}sent_tokenize_bm.avro\")\n",
    "# ################################################################################################################\n",
    "# ## preprocessing III\n",
    "df = read_avro(spark, f\"{hdfs_working_path}p2_bm\")\n",
    "sen = get_sentence(df, \"text\").select(\"text\")\n",
    "flat = sen.select(flatten(sen.text).alias('text'))\n",
    "a = flat.rdd.map(lambda x : (' '.join(x[0]),1)).toDF([\"text\", \"id\"]).drop(\"id\")\n",
    "\n",
    "print(\"================ SAVE THE FINAL PROCESS SENTENCE DATA ================\")\n",
    "write_avro(a, f\"{hdfs_working_path}final_process_bm\")\n",
    "remove_from_hdfs(f\"{hdfs_working_path}p2_bm\")\n",
    "# # ################################################################################################################\n",
    "# # ## Word count statistics\n",
    "df = read_avro(spark, f\"{hdfs_working_path}final_process_bm\")\n",
    "malayStopWords = set(json.load(open(malay_stopword_path)))\n",
    "stopwordsSet = set(stopwords.words('english') + stopwords.words('chinese') ).union(malayStopWords)\n",
    "stopWordsRemoval = StopwordsFilteringTool(stopwordsSet)\n",
    "nsbm = stopWordsRemoval.get_no_stop_word_df(df, \"text\")\n",
    "wc = \\\n",
    "    nsbm.rdd.flatMap(lambda x : x.text) \\\n",
    "    .map(lambda x : (x,1)).reduceByKey(lambda V1, V2 : V1+V2) \\\n",
    "    .toDF([\"word\",\"word_count\"])\n",
    "\n",
    "wc\\\n",
    "    .select(\"word\",\"word_count\", F.row_number() \\\n",
    "            .over(Window.partitionBy().orderBy(desc(wc['word_count']))) \\\n",
    "            .alias(\"id\")).show(1000)\n",
    "# # ################################################################################################################\n",
    "# # ## Prompt user to enter the keywords\n",
    "# # ## Select topics\n",
    "keys = []\n",
    "want_choose = True\n",
    "while want_choose:\n",
    "    key = input(\"enter keyword : \")\n",
    "    keys.append(key)\n",
    "    decision = input(\"Again? (y / n) :  \")\n",
    "    if not (decision.upper() == 'Y' or decision.upper() == 'YES'):\n",
    "        want_choose = False\n",
    "print(\"your selected keys is \" + str(keys))\n",
    "# # ################################################################################################################\n",
    "regex = \"|\".join(keys)\n",
    "filtered = df.filter( df.text.rlike(regex))\n",
    "\n",
    "print(\"================ SAVE THE FILTERED DATA ================\")\n",
    "write_avro(filtered, f\"{hdfs_working_path}filtered_bm_data\")\n",
    "# # ################################################################################################################\n",
    "# # ## create sentence word count statistics\n",
    "df = read_avro(spark, f\"{hdfs_working_path}filtered_bm_data\")\n",
    "with_statistics = df.withColumn(\"token\", u.tokenize_chineseDF(df.text))\n",
    "with_statistics = with_statistics.withColumn(\"length\", size(with_statistics.token))\n",
    "df = with_statistics.select(\"text\", \"length\")\n",
    "\n",
    "print(\"==========SENTENCE WITH WORD COUNT==========\")\n",
    "write_avro(df, hdfs_working_path+\"bm_sentence_with_statistics\")\n",
    "remove_from_hdfs(hdfs_working_path+\"filtered_bm_data\")\n",
    "# # ################################################################################################################\n",
    "# # ## remove the outliers\n",
    "s = read_avro(spark, hdfs_working_path+\"bm_sentence_with_statistics\")\n",
    "s = s.coalesce(74)\n",
    "sC = s.select(s.length)\n",
    "sC.toPandas().plot.box()\n",
    "plt.show()\n",
    "s_detect = find_outliers(s)\n",
    "noS = s_detect.filter((s_detect.total_outliers < 1)  & (s_detect.length > 3))\n",
    "sC = noS.select(noS.length)\n",
    "sC.toPandas().plot.box()\n",
    "plt.show()\n",
    "\n",
    "print(\"==========SENTENCE WITHOUT OUTLIERS==========\")\n",
    "write_avro(noS, hdfs_working_path+\"bm_sentence_without_out\")\n",
    "remove_from_hdfs(hdfs_working_path+\"bm_sentence_with_statistics\")\n",
    "# ################################################################################################################\n",
    "# ## pos tag\n",
    "df = read_avro(spark, hdfs_working_path+\"bm_sentence_without_out\")\n",
    "df = assign_id_column(df)\n",
    "write_avro(df, hdfs_working_path+\"bm_with_id\")\n",
    "df = read_avro(spark, hdfs_working_path+\"bm_with_id\")\n",
    "\n",
    "numberOfRows = df.count()\n",
    "splitInto = math.ceil(df.count()/1000)\n",
    "segments = get_segments(df, splitInto)\n",
    "\n",
    "\n",
    "\n",
    "# split to smaller file to easier processing\n",
    "for i in range(splitInto):\n",
    "    print(\"write \" + str(i))\n",
    "    write_avro(segments[i].select(\"text\"), f\"IN/{i}\")\n",
    "    print(\"LEN = \" + str(segments[i].count()))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "print(f\"the file is splitted into {splitInto} files to ease the prcessing\")\n",
    "\n",
    "import malaya\n",
    "from pyspark.sql.functions import col, udf, size, split\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "def posTagBMBI(text):\n",
    "    try:\n",
    "        model\n",
    "    except:\n",
    "        model = malaya.pos.transformer(model = 'albert', quantized=False, gpu=False)\n",
    "        \n",
    "    try:\n",
    "        r = model.predict(text)\n",
    "        return [x[0] for x in r],[x[1] for x in r]\n",
    "    except:\n",
    "        return \"\",\"\"\n",
    "\n",
    "posUDF = udf(lambda x : posTagBMBI(x), ArrayType(ArrayType(StringType())))\n",
    "\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "for i in range(splitInto):\n",
    "    df = read_avro(spark, f\"IN/{i}\")\n",
    "    df.show(5, False)\n",
    "    tag = df.withColumn(\"tag\", posUDF(df.text))\n",
    "    write_avro(tag, f\"OUT/{i}\")\n",
    "    spark.catalog.clearCache()\n",
    "    print(\"done execution !\")    \n",
    "\n",
    "\n",
    "    \n",
    "import shutil\n",
    "shutil.rmtree(\"IN\")\n",
    "\n",
    "\n",
    "## merge the file to one and remove the out directory\n",
    "alt = Alternator()\n",
    "\n",
    "\n",
    "for i in range(splitInto):\n",
    "    if i == 0:\n",
    "        merge = read_avro(spark, f\"OUT/{i}\")\n",
    "        write_avro(merge, f\"Merge/{alt.num}\")\n",
    "    else:\n",
    "        merge = read_avro(spark, f\"Merge/{alt.num}\")\n",
    "        df = read_avro(spark, f\"OUT/{i}\")\n",
    "        union = unionAll(merge, df)\n",
    "        alt.alternate()\n",
    "        write_avro(union, f\"Merge/{alt.num}\")\n",
    "        remove_from_hdfs(f\"Merge/{alt.get_alternate()}\")\n",
    "\n",
    "shutil.rmtree(\"OUT\")\n",
    "df = read_avro(spark, f\"Merge/{alt.num}\")\n",
    "df.show(10, False)\n",
    "# ################################################################################################################\n",
    "# Standardize the format\n",
    "df = df.rdd.map(lambda x : (x.text, x.tag[0], x.tag[1])).\\\n",
    "        toDF([\"original\",\"token\",\"tag\"])\n",
    "write_avro(df, f\"{hdfs_working_path}BM_tokenAndTag\")\n",
    "# ################################################################################################################\n",
    "## create n-gram\n",
    "a = read_avro(spark, hdfs_working_path+\"BM_tokenAndTag\")\n",
    "a = a.repartition(60)\n",
    "a= a.withColumn(\"sentence_id\", F.monotonically_increasing_id())\n",
    "a = a.rdd.map(lambda x : (x.sentence_id,\" \".join(x.token), x.token, x.tag)).toDF([\"sentence_id\", \"original\",\"token\",\"tag\"])\n",
    "write_avro(a, hdfs_working_path+\"bm_sentence_with_tagging_format\")\n",
    "remove_from_hdfs(hdfs_working_path+\"BM_tokenAndTag\")\n",
    "# ################################################################################################################\n",
    "a = read_avro(spark, hdfs_working_path+\"bm_sentence_with_tagging_format\")\n",
    "# ################################################################################################################\n",
    "# ## bigram\n",
    "tokenNgram =NGram(n=2, inputCol=\"token\", outputCol=\"token_gram\")\n",
    "tagNgram =NGram(n=2, inputCol=\"tag\", outputCol=\"tag_gram\")\n",
    "n2 = tokenNgram.transform(a)\n",
    "n2 = tagNgram.transform(n2)\n",
    "t = n2.withColumn(\"tmp\", arrays_zip(\"token_gram\", \"tag_gram\"))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\",\"token\", F.posexplode(col(\"tmp\")))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\", \"token\", \"pos\", col(\"col.token_gram\"), col(\"col.tag_gram\"))\n",
    "\n",
    "print(\"==========CREATED BIGRAM==========\")\n",
    "write_avro(t, hdfs_working_path+\"BM_N2_GRAM\")\n",
    "# ################################################################################################################\n",
    "# ## trigram\n",
    "tokenNgram =NGram(n=3, inputCol=\"token\", outputCol=\"token_gram\")\n",
    "tagNgram =NGram(n=3, inputCol=\"tag\", outputCol=\"tag_gram\")\n",
    "\n",
    "n2 = tokenNgram.transform(a)\n",
    "n2 = tagNgram.transform(n2)\n",
    "\n",
    "t = n2.withColumn(\"tmp\", arrays_zip(\"token_gram\", \"tag_gram\"))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\",\"token\", F.posexplode(col(\"tmp\")))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\", \"token\", \"pos\", col(\"col.token_gram\"), col(\"col.tag_gram\"))\n",
    "\n",
    "\n",
    "print(\"==========CREATED TRIGRAM==========\")\n",
    "write_avro(t, hdfs_working_path+\"BM_N3_GRAM\")\n",
    "# ################################################################################################################\n",
    "## n4 gram\n",
    "tokenNgram =NGram(n=4, inputCol=\"token\", outputCol=\"token_gram\")\n",
    "tagNgram =NGram(n=4, inputCol=\"tag\", outputCol=\"tag_gram\")\n",
    "\n",
    "n2 = tokenNgram.transform(a)\n",
    "n2 = tagNgram.transform(n2)\n",
    "\n",
    "t = n2.withColumn(\"tmp\", arrays_zip(\"token_gram\", \"tag_gram\"))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\",\"token\", F.posexplode(col(\"tmp\")))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\", \"token\", \"pos\", col(\"col.token_gram\"), col(\"col.tag_gram\"))\n",
    "\n",
    "\n",
    "print(\"==========CREATED N4 GRAM==========\")\n",
    "write_avro(t, hdfs_working_path+\"BM_N4_GRAM\")\n",
    "# ################################################################################################################\n",
    "## n5 gram\n",
    "tokenNgram =NGram(n=5, inputCol=\"token\", outputCol=\"token_gram\")\n",
    "tagNgram =NGram(n=5, inputCol=\"tag\", outputCol=\"tag_gram\")\n",
    "\n",
    "n2 = tokenNgram.transform(a)\n",
    "n2 = tagNgram.transform(n2)\n",
    "\n",
    "t = n2.withColumn(\"tmp\", arrays_zip(\"token_gram\", \"tag_gram\"))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\",\"token\", F.posexplode(col(\"tmp\")))\\\n",
    ".select(\"sentence_id\", \"original\", \"tag\", \"token\", \"pos\", col(\"col.token_gram\"), col(\"col.tag_gram\"))\n",
    "\n",
    "\n",
    "print(\"==========CREATED N5 GRAM==========\")\n",
    "write_avro(t, hdfs_working_path+\"BM_N5_GRAM\")\n",
    "# ################################################################################################################\n",
    "# ## merge ngram\n",
    "n2 = read_avro(spark, hdfs_working_path+\"BM_N2_GRAM\")\n",
    "n3 = read_avro(spark, hdfs_working_path+\"BM_N3_GRAM\")\n",
    "n4 = read_avro(spark, hdfs_working_path+\"BM_N4_GRAM\")\n",
    "n5 = read_avro(spark, hdfs_working_path+\"BM_N5_GRAM\")\n",
    "\n",
    "\n",
    "n2 = n2.withColumn(\"gram_type\", lit(2))\n",
    "n3 = n3.withColumn(\"gram_type\", lit(3))\n",
    "n4 = n4.withColumn(\"gram_type\", lit(4))\n",
    "n5 = n5.withColumn(\"gram_type\", lit(5))\n",
    "\n",
    "\n",
    "merge = unionAll(n2, n3, n4, n5)\n",
    "\n",
    "print(\"==========MERGED THE NGRAMS==========\")\n",
    "write_avro(merge, hdfs_working_path+\"BM_MERGE_GRAM\")\n",
    "# ################################################################################################################\n",
    "# ## gram frequency statistics\n",
    "# ## Tag frequency\n",
    "# ################################################################################################################\n",
    "# ## filter n gram with key words\n",
    "df = read_avro(spark, hdfs_working_path+\"BM_MERGE_GRAM\")\n",
    "keys = set(keys)\n",
    "keyGramFilter = KeyGram(keys)\n",
    "withKeyOnly = keyGramFilter.getDFwithKeywordsOnly(df)\n",
    "\n",
    "print(\"==========GET NGRAM WITH KEYWORDS ONLY==========\")\n",
    "write_avro(withKeyOnly, f\"{hdfs_working_path}BM_KeyOnlyData\")\n",
    "# ################################################################################################################\n",
    "# create the column to identify whether keyword at middle\n",
    "df = read_avro(spark,  f\"{hdfs_working_path}BM_KeyOnlyData\")\n",
    "mid = MiddleKey(keys)\n",
    "withMiddleFlag = df.withColumn(\"middle_key\", mid.middleUDF(df.gram_type, df.token_gram))\n",
    "\n",
    "print(\"==========MARK THE NGRAM WITH KEYWORDS MIDDLE ON IT==========\")\n",
    "write_avro(withMiddleFlag, f\"{hdfs_working_path}BM_MiddleFlagData\")\n",
    "remove_from_hdfs(f\"{hdfs_working_path}BM_KeyOnlyData\")\n",
    "# ################################################################################################################\n",
    "# ## Frequency statistics\n",
    "# ## tag frequency\n",
    "df = read_avro(spark,  f\"{hdfs_working_path}BM_MiddleFlagData\")\n",
    "tagFreq = df.groupBy(\"tag_gram\").count().withColumnRenamed(\"count\", \"tag_gram_f\")\n",
    "write_avro(tagFreq, hdfs_working_path+\"BM_TAG_FREQ\")\n",
    "tagFreq = read_avro(spark, hdfs_working_path+\"BM_TAG_FREQ\")\n",
    "withtagFBI = df.join(tagFreq, df.tag_gram == tagFreq.tag_gram).drop(tagFreq.tag_gram)\n",
    "\n",
    "print(\"==========COMPUTED THE TAG FREQUENCY==========\")\n",
    "write_avro(withtagFBI, hdfs_working_path+\"BM_TAG_F\")\n",
    "remove_from_hdfs(hdfs_working_path+\"BM_TAG_FREQ\")\n",
    "# ################################################################################################################\n",
    "# ## Token frequency\n",
    "df = read_avro(spark, hdfs_working_path+\"BM_TAG_F\")\n",
    "bitokenFreq = df.groupBy(\"token_gram\").count().withColumnRenamed(\"count\", \"token_gram_f\")\n",
    "write_avro(bitokenFreq, hdfs_working_path+\"BM_TOKEN_FREQ\")\n",
    "tokenFreq = read_avro(spark, hdfs_working_path+\"BM_TOKEN_FREQ\")\n",
    "biwithTokF = df.join(tokenFreq, df.token_gram == tokenFreq.token_gram).drop(tokenFreq.token_gram)\n",
    "\n",
    "print(\"==========COMPUTED THE TOKEN FREQUENCY==========\")\n",
    "write_avro(biwithTokF, hdfs_working_path+\"BM_TOKEN_F\")\n",
    "remove_from_hdfs(hdfs_working_path+\"BM_TOKEN_FREQ\")\n",
    "remove_from_hdfs(hdfs_working_path+\"BM_TAG_F\")\n",
    "# ################################################################################################################\n",
    "# ## Table Normalization\n",
    "# ## Gram table\n",
    "df = read_avro(spark, hdfs_working_path+\"BM_TOKEN_F\")\n",
    "df = df.repartition(800)\n",
    "gram_table = df.select(\"sentence_id\", \"pos\", \"token_gram\", \"token_gram_f\", \"tag_gram\", \"tag_gram_f\", \"gram_type\", \"containsKey\", \"middle_key\")\n",
    "gram_table = gram_table.withColumn(\"uniqueID\", monotonically_increasing_id())\n",
    "write_avro(gram_table, hdfs_working_path+\"BM_GRAM_TABLE\")\n",
    "# ################################################################################################################\n",
    "# ## Source Table\n",
    "originalSentence  = df.dropDuplicates((['sentence_id'])).select(\"sentence_id\", \"original\", \"tag\", \"token\")\n",
    "originalSentence = originalSentence.repartition(800)\n",
    "print(\"if disk block error happen, try increase the value of repartition\")\n",
    "originalSentence = originalSentence.withColumn(\"uniqueID\", monotonically_increasing_id())\n",
    "write_avro(originalSentence, hdfs_working_path+\"BM_SOURCE_TABLE\")\n",
    "# ################################################################################################################\n",
    "# ## Into Hive\n",
    "gram_table = read_avro(spark, hdfs_working_path+\"BM_GRAM_TABLE\")\n",
    "original_table = read_avro(spark, hdfs_working_path+\"BM_SOURCE_TABLE\")\n",
    "\n",
    "spark.sql(\"create database if not exists wikipedia_db;\")\n",
    "spark.sql(\"use wikipedia_db;\")\n",
    "spark.sql(\"drop table if exists gram_table_ms;\")\n",
    "spark.sql(\"drop table if exists source_data_ms;\")\n",
    "\n",
    "\n",
    "gram_table.write\\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"wikipedia_db.gram_table_ms\")\n",
    "\n",
    "\n",
    "original_table.write\\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"wikipedia_db.source_data_ms\")\n",
    "\n",
    "# ################################################################################################################\n",
    "# ## Analytics\n",
    "# # analytics : Top 10 Common Pos Pattern\n",
    "Q=\"\"\"\n",
    "select\n",
    "    distinct tag_gram,\n",
    "    tag_gram_f\n",
    "from\n",
    "    gram_table_ms\n",
    "order by\n",
    "    tag_gram_f DESC limit 10;\n",
    "\"\"\"\n",
    "\n",
    "SMPos = spark.sql(Q).toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,5))\n",
    "\n",
    "x = SMPos['tag_gram'][:10].tolist()\n",
    "y = SMPos['tag_gram_f'][:10].tolist()\n",
    "ax.barh(x,y)\n",
    "ax.set_title('Top 10 Most Common POS Patterns in Wikipedia Malay')\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(value, index,\n",
    "             str(value))\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.show()\n",
    "# ################################################################################################################\n",
    "# ## Analytics\n",
    "# # analytics : Top 10 Common Token Pattern\n",
    "Q=\"\"\"\n",
    "select\n",
    "    distinct token_gram,\n",
    "    token_gram_f\n",
    "from\n",
    "    gram_table_ms\n",
    "order by\n",
    "    token_gram_f DESC limit 10;\n",
    "\"\"\"\n",
    "\n",
    "SMtok = spark.sql(Q).toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,5))\n",
    "\n",
    "x = SMtok['token_gram'][:10].tolist()\n",
    "y = SMtok['token_gram_f'][:10].tolist()\n",
    "ax.barh(x,y)\n",
    "ax.set_title('Top 10 Most Common Token Patterns in Wikipedia Malay')\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(value, index,\n",
    "             str(value))\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867d55b0-2d38-4031-83c1-2ebe0a63972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## arguments\n",
    "pyspark_bin_path = \"/home/pc/TestJupyter/opt/spark-3.3.0/spark-3.3.0-bin-hadoop3\"\n",
    "python_path = \"/home/pc/anaconda3/envs/conda_env/bin/python3.7\"\n",
    "app_name = \"social media\"\n",
    "data_path = \"/home/pc/data/parsed_data/2parsed-extract-mswiki.txt\"\n",
    "hdfs_working_path = \"hdfs://10.123.51.78:8020/user/wikipedia/\"\n",
    "malay_stopword_path = \"/home/pc/Assignment/node_modules/stopwords-ms/stopwords-ms.json\"\n",
    "jieba_replace_csv = \"/home/pc/Assignment/SocialMedia/Main/replacement.csv\"\n",
    "hdfsHost = \"hdfs://10.123.51.78\"\n",
    "hdfsPort = \"8020\"\n",
    "hdfs_working_path_file = \"/user/wikipedia/\"\n",
    "hiveWarehouseDirectory = \"hdfs://10.123.51.78:8020/user/hive/warehouse\"\n",
    "hiveTriftServerAddress = \"thrift://g2.bigtop.it:9083\"\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import findspark\n",
    "findspark.init(pyspark_bin_path)\n",
    "from FileIO import *\n",
    "import utilities as u\n",
    "import pyspark\n",
    "from Preprocessing import *\n",
    "from pyspark.sql import SparkSession\n",
    "from DetectLanguage import *\n",
    "from LangAgg import *\n",
    "from WordCount import *\n",
    "from pyspark.sql.window import Window\n",
    "from nltk.corpus import stopwords\n",
    "from FilterOutliers import *\n",
    "from postagChinese import *\n",
    "from JiebaTagToStandardTag import *\n",
    "from SegmentDataframe import *\n",
    "from Alternator import *\n",
    "from TokenLanguageReplaceTool import *\n",
    "from pyspark.ml.feature import NGram\n",
    "from KeyGram import *\n",
    "from MiddleKey import *\n",
    "from StopwordsFilteringTool import *\n",
    "from SeqID import *\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "## initialize spark session\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"hdfs\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = python_path\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"70g\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", hiveWarehouseDirectory) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\\\n",
    "    .config(\"hive.metastore.uris\", hiveTriftServerAddress) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = read_avro(spark, \"IN/1.avro\")\n",
    "df.write.mode(\"overwrite\").parquet(\"IN/1.parquet\")\n",
    "df = spark.read.parquet(\"IN/1.parquet\")\n",
    "\n",
    "df = df.repartition(54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43aa444-4a47-4d8a-b535-b57e2e858119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 21:45:51.737135: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 21:45:51.881519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:45:51.881552: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-10 21:45:51.918260: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-10 21:45:52.346546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:45:52.346631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:45:52.346640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya_boilerplate/frozen_graph.py:36: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  f'Cannot import beam_search_ops from Tensorflow Addons, {USED_TREE} will not available to use, make sure Tensorflow Addons version >= 0.12.0'\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya_boilerplate/frozen_graph.py:39: UserWarning: check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases\n",
      "  'check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases')\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3361\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3879\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "import malaya\n",
    "from pyspark.sql.functions import col, udf, size, split\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "model = None\n",
    "def posTagBMBI(text):\n",
    "    global model\n",
    "    if model == None:\n",
    "        model = malaya.pos.transformer(model = 'albert', quantized=False)\n",
    "        r = model.predict(text)\n",
    "        return [x[0] for x in r],[x[1] for x in r]\n",
    "    else:\n",
    "        r = model.predict(text)\n",
    "        return [x[0] for x in r],[x[1] for x in r]\n",
    "    print(\"qdone\")\n",
    "\n",
    "posUDF = udf(lambda x : posTagBMBI(x), ArrayType(ArrayType(StringType())))\n",
    "\n",
    "\n",
    "df = df.filter(df.text != \"\")\n",
    "df = df.withColumn(\"tag\", posUDF(df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9ffce-1d19-4999-8158-d299428cc93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 21:46:06.500097: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 21:46:06.607832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:46:06.607865: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-10 21:46:06.633297: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-10 21:46:07.103438: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:46:07.103503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:46:07.103511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya_boilerplate/frozen_graph.py:36: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  f'Cannot import beam_search_ops from Tensorflow Addons, {USED_TREE} will not available to use, make sure Tensorflow Addons version >= 0.12.0'\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya_boilerplate/frozen_graph.py:39: UserWarning: check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases\n",
      "  'check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases')\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3361\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/pc/anaconda3/envs/conda_env/lib/python3.7/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3879\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "2022-10-10 21:46:12.543217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-10 21:46:12.545043: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-10 21:46:12.545076: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-10 21:46:12.545107: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (g2.bigtop.it): /proc/driver/nvidia/version does not exist\n",
      "2022-10-10 21:46:13.233070: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c863f-cfb7-4ee8-85ea-bd5030ef8a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "conda_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
